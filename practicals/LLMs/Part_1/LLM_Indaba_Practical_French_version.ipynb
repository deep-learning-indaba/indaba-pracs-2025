{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2s4kN_QPQVe"
   },
   "source": [
    "# Pratique LLM Indaba 2025\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/take-memories/images/f_auto,dpr_auto,q_auto,w_2000,c_fill,h_1200/gm/hbb8oblj5tozmimydbaz/rwanda-sehenswurdigkeiten\" width=\"60%\"/>\n",
    "\n",
    "¬© Deep Learning Indaba 2025. Licence Apache 2.0.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2025/blob/main/practicals/LLMs/Part_1/LLM_Indaba_Practical_French_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Colab\"/></a>\n",
    "\n",
    "**Auteurs :** Tejumade Afonja, Qurat Ul Ain (Annie), Jabez Magomere, Abla Hagani, Amel Sellami, Massimo Nicosia, Sebastian Bodenstein.\n",
    "\n",
    "**Relecteurs :** Ulrich A. Mbou Sob, Siddarth Singh, Sasha Abramowitz, Ruan de Kock.\n",
    "\n",
    "**Traducteurs** Amel Sellami, Abla Hagani.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Les grands mod√®les de langage (LLMs) comme ChatGPT et Gemini ont r√©volutionn√© le domaine du traitement automatique du langage naturel, mais comprendre leur fonctionnement interne peut √™tre complexe. Ce cours pratique est con√ßu pour d√©mystifier les concepts cl√©s derri√®re ces mod√®les, en commen√ßant par l‚Äôid√©e fondamentale de l‚Äôattention, puis en explorant l‚Äôarchitecture qui alimente les syst√®mes les plus avanc√©s d‚Äôaujourd‚Äôhui. Au fil du parcours, vous acquerrez des connaissances pratiques sur la mani√®re dont la langue et la tokenisation influencent le comportement et les co√ªts des mod√®les, et vous entra√Ænerez m√™me votre propre petit mod√®le de langage (SLM).\n",
    "\n",
    "Tout au long de cette pratique, nous couvrirons les domaines cl√©s suivants :\n",
    "\n",
    "1. **Chargement et interaction avec les LLMs :** Exp√©rimentez directement avec des mod√®les pr√©-entra√Æn√©s de Hugging Face et comprenez comment g√©n√©rer du texte et contr√¥ler la sortie via diff√©rents param√®tres.\n",
    "\n",
    "2. **Explorer les applications concr√®tes :** D√©couvrez comment les LLMs sont utilis√©s dans diverses t√¢ches telles que la g√©n√©ration de code, la r√©ponse aux questions, l‚Äô√©criture cr√©ative, et la r√©ponse aux questions visuelles.\n",
    "3. **Architecture Transformer :** Plongez dans les √©l√©ments constitutifs des LLMs modernes, incluant un rappel rapide de l‚Äôarchitecture Transformer et de ses composants cl√©s comme l‚Äôattention automatique (self-attention) et l‚Äôattention multi-t√™tes (multi-head attention).\n",
    "4. **Tokenisation et embeddings :** Apprenez comment le texte est converti en repr√©sentations num√©riques compr√©hensibles par les LLMs, et explorez l‚Äôimpact des diff√©rentes strat√©gies de tokenisation selon les langues.\n",
    "5. **Entra√Ænement de votre propre LLM :** Impl√©mentez et entra√Ænez un mod√®le d√©codeur Transformer simplifi√© √† partir de z√©ro en utilisant un jeu de donn√©es des ≈ìuvres de Shakespeare.\n",
    "\n",
    "6. **Le co√ªt de la langue :** Analysez comment la tokenisation peut affecter le co√ªt d‚Äôutilisation des LLMs commerciaux, particuli√®rement selon les diff√©rentes langues.\n",
    "\n",
    "Cette pratique est adapt√©e aux personnes ayant un niveau d√©butant √† interm√©diaire en apprentissage profond et en traitement du langage naturel. Nous recommandons d‚Äôavoir une compr√©hension de base en alg√®bre lin√©aire.\n",
    "\n",
    "Commen√ßons !\n",
    "\n",
    "**Sujets :**\n",
    "\n",
    "Contenu : \\[<font color='orange'>Introduction √† Hugging Face</font>, <font color='orange'>Interagir avec les LLMs</font>, <font color='orange'>Tokenisation</font>, <font color='orange'>Embeddings</font>, <font color='orange'>Architecture Transformer</font>, <font color='green'>M√©canisme d‚ÄôAttention</font>, <font color='green'>Entra√Æner votre propre LLM depuis z√©ro</font>]\n",
    "\n",
    "Niveau : <font color='orange'>D√©butant</font>, <font color='green'>Interm√©diaire</font>\n",
    "\n",
    "**Objectifs d‚Äôapprentissage :**\n",
    "\n",
    "* Comprendre le concept derri√®re [l‚Äôattention](https://arxiv.org/abs/1706.03762) et pourquoi elle est utilis√©e.\n",
    "* Pr√©senter et d√©crire les blocs fondamentaux de l‚Äô[architecture Transformer](https://arxiv.org/abs/1706.03762) ainsi qu‚Äôune intuition sur la conception de cette architecture.\n",
    "* Comparer les tokenizers √† travers diff√©rentes langues et analyser comment ces diff√©rences influencent les co√ªts mon√©taires associ√©s.\n",
    "* Construire et entra√Æner votre propre LLM.\n",
    "\n",
    "**Pr√©requis :**\n",
    "\n",
    "* Connaissances de base en apprentissage profond.\n",
    "* Familiarit√© avec le traitement du langage naturel (NLP).\n",
    "* Compr√©hension de base en alg√®bre lin√©aire.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM-X0-PoZOAB"
   },
   "source": [
    "**Plan:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "7_PaafMKZFHb"
   },
   "source": [
    "# [Pratique LLM Indaba 2025](#scrollTo=m2s4kN_QPQVe)\n",
    "\n",
    ">>[Installation et configuration [D√©butant]](#scrollTo=Zgn0dW3IH9NQ)\n",
    "\n",
    ">>>[[Lancez-moi] Installations, Imports et Fonctions utilitaires](#scrollTo=6EqhIg1odqg0)\n",
    "\n",
    ">>[ü§ñ Charger un mod√®le depuis Hugging Face et interagir localement [D√©butant]](#scrollTo=ElXJ1BGzH5BJ)\n",
    "\n",
    ">>>[üéØ Objectif](#scrollTo=8Lrb9vL2uO5A)\n",
    "\n",
    ">>>>>[üß† Qu'est-ce que les grands mod√®les de langage ?](#scrollTo=me-nBsxtGBA1)\n",
    "\n",
    ">>>>[√Ä propos de HuggingFace](#scrollTo=ltfz9jstGSTJ)\n",
    "\n",
    ">>>>>[Votre premier mod√®le de langage](#scrollTo=M1e-FfGzGrr_)\n",
    "\n",
    ">>>>[Comprendre les param√®tres de g√©n√©ration](#scrollTo=5nflrXCXHY6u)\n",
    "\n",
    ">>>[Temp√©rature](#scrollTo=5nflrXCXHY6u)\n",
    "\n",
    ">>>[Top-p (√âchantillonnage par noyau)](#scrollTo=5nflrXCXHY6u)\n",
    "\n",
    ">>>>[Mod√®les de langage dans des applications r√©elles](#scrollTo=J5U3YbK_IGel)\n",
    "\n",
    ">>[üîç R√©capitulatif rapide de l‚Äôarchitecture Transformer [D√©butant]](#scrollTo=svfeQO7VIOIu)\n",
    "\n",
    ">>>[Vue d'ensemble du d√©codeur Transformer](#scrollTo=brwKNHT0-Hhl)\n",
    "\n",
    ">>[üß± Tokenisation [D√©butant]](#scrollTo=AtWMaTddww65)\n",
    "\n",
    ">>>[üéØ Essayez par vous-m√™me : Terrain de jeu du tokenizer](#scrollTo=_Ku_PEI0PF4l)\n",
    "\n",
    ">>>[Jouez avec le tokenizer Gemma](#scrollTo=p6qNp4IhGP-K)\n",
    "\n",
    ">>[ìä≥ Embeddings [D√©butant]](#scrollTo=9YPYutB1TAes)\n",
    "\n",
    ">>[‚ïë Codages positionnels : pourquoi l‚Äôordre compte [D√©butant]](#scrollTo=WNO703V9SBcI)\n",
    "\n",
    ">>>>>[Fonctions sinus et cosinus : une fa√ßon simple d‚Äôajouter des informations de position](#scrollTo=nxkDif_aRGKy)\n",
    "\n",
    ">>[Les embeddings informent l‚Äôattention [D√©butant]](#scrollTo=Qkh0KgRPdDf8)\n",
    "\n",
    ">>[üîç Attention [Interm√©diaire]](#scrollTo=vY02IFQouwjN)\n",
    "\n",
    ">>>[Entre self-attention et multi-head attention](#scrollTo=_qJdLHPBL1I8)\n",
    "\n",
    ">>>[Self-Attention](#scrollTo=TUPfggF9L9tE)\n",
    "\n",
    ">>>[Attention masqu√©e](#scrollTo=yJ4lTjELMj68)\n",
    "\n",
    ">>>[La b√™te aux multiples t√™tes : Multi-Head Attention](#scrollTo=X31b1Pt6MvJ8)\n",
    "\n",
    ">>>[Attention par produit scalaire mis √† l‚Äô√©chelle](#scrollTo=WN0q3iq9SMdn)\n",
    "\n",
    ">>[√Ä garder √† l‚Äôesprit :](#scrollTo=nF3tNzT_NGIm)\n",
    "\n",
    ">>[üèóÔ∏è Entra√Ænement de votre propre LLM (Transformers) [Interm√©diaire]](#scrollTo=5X4tRtSZxGHg)\n",
    "\n",
    ">>>[Objectif](#scrollTo=5X4tRtSZxGHg)\n",
    "\n",
    ">>>[Bloc Transformer [Interm√©diaire]](#scrollTo=e71jR6TYRHEP)\n",
    "\n",
    ">>>>[R√©seau Feed Forward (FFN) / Perceptron multicouche (MLP) [D√©butant]](#scrollTo=5yAG_MbgRWEs)\n",
    "\n",
    ">>>>[Bloc Add & Norm [D√©butant]](#scrollTo=J2Us0NGFRUPn)\n",
    "\n",
    ">>>[Construction du d√©codeur Transformer / LLM [Interm√©diaire]](#scrollTo=i0Z_7oRRRqPg)\n",
    "\n",
    ">>>[Entra√Ænement de votre LLM](#scrollTo=7nsFaXhdSKZG)\n",
    "\n",
    ">>>>[Objectif d‚Äôentra√Ænement [Interm√©diaire]](#scrollTo=o6BUm34sSRJH)\n",
    "\n",
    ">>>>[Mod√®les d‚Äôentra√Ænement [Interm√©diaire]](#scrollTo=7Jp_1cbQSnzq)\n",
    "\n",
    ">>>>[Inspection du LLM entra√Æn√© [D√©butant]](#scrollTo=qE5N87UWT_uK)\n",
    "\n",
    ">>[√Ä m√©diter : combien co√ªte une conversation avec un LLM dans votre langue ?](#scrollTo=tixtBEtRPZ5n)\n",
    "\n",
    ">>>[Calculons le co√ªt des tokens](#scrollTo=tixtBEtRPZ5n)\n",
    "\n",
    ">>>>[üí∞ Estimations d‚Äôexemple :](#scrollTo=tixtBEtRPZ5n)\n",
    "\n",
    ">>>>[üí∏ Combien co√ªte ma langue ? ‚Äî Tokenisation en code](#scrollTo=QVTduxk4PdYC)\n",
    "\n",
    ">>>>[üßµ Points cl√©s](#scrollTo=WwCo9941QRo2)\n",
    "\n",
    ">[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
    "\n",
    ">[Retour d‚Äôexp√©rience](#scrollTo=o1ndpYE50BpG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOJzzH88ZJL6"
   },
   "source": [
    "**Avant** de commencer:\n",
    "\n",
    "Pour cela, vous devrez utiliser un GPU pour acc√©l√©rer l'entra√Ænement.Pour ce faire, acc√©dez au menu \"Runtime\" dans Colab, s√©lectionnez \"Modifier le type d'ex√©cution\", puis dans le menu contextuel, choisissez \"GPU\" dans la case \"Accelerator mat√©riel\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "952qogb79nnY"
   },
   "source": [
    "***Niveau** d'exp√©rience sugg√©r√© dans ce sujet:\n",
    "\n",
    "| Level         | Experience                            |\n",
    "| --- | --- |\n",
    "`D√©butant`      | C‚Äôest la premi√®re fois que je d√©couvre ce domaine. |\n",
    "`Interm√©diaire` | J‚Äôai suivi quelques cours ou introductions de base sur ce sujet. |\n",
    "`Avanc√©`        | Je travaille dans ce domaine/sujet au quotidien. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgn0dW3IH9NQ"
   },
   "source": [
    "## Installation et configuration [<font color = 'orange'> d√©butant </font>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YBdDHcI_ArCR"
   },
   "outputs": [],
   "source": [
    "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
    "experience = 'beginner' #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "sections_to_follow=''\n",
    "\n",
    "\n",
    "if experience == 'beginner': sections_to_follow = '''Nous vous recommandons de ne pas essayer de r√©aliser toutes les t√¢ches de codage, mais plut√¥t de parcourir chaque section et de vous assurer d‚Äôex√©cuter chaque cellule afin d‚Äôacqu√©rir une compr√©hension pratique du comportement de ces mod√®les.'''\n",
    "\n",
    "elif experience == 'intermediate': sections_to_follow = '''\n",
    "Nous vous recommandons de parcourir chaque section de ce notebook et d‚Äôessayer les t√¢ches de codage marqu√©es comme d√©butant ou interm√©diaire. Si vous bloquez sur le code, demandez de l‚Äôaide √† un tuteur ou passez √† une meilleure utilisation du temps de la pratique.\n",
    "'''\n",
    "\n",
    "elif experience == 'advanced': sections_to_follow = '''Nous vous recommandons de parcourir chaque section et d‚Äôessayer chaque t√¢che de codage jusqu‚Äô√† ce que vous l‚Äôobteniez √† fonctionner.'''\n",
    "\n",
    "\n",
    "print(f'D‚Äôapr√®s votre exp√©rience, {sections_to_follow}.\\n Note : ceci est juste une recommandation, n‚Äôh√©sitez pas √† explorer le colab comme bon vous semble si vous vous sentez √† l‚Äôaise !')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EqhIg1odqg0"
   },
   "source": [
    "### [Ex√©cutez-moi] Installations, importations et fonctions d'assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwvNaDj1VrPp"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "required_version = (3, 11)\n",
    "current_version = sys.version_info[:2]\n",
    "\n",
    "if current_version != required_version:\n",
    "    print(f\"‚ö†Ô∏è Warning: Expected Python {required_version[0]}.{required_version[1]}, but running {current_version[0]}.{current_version[1]}. Some package may not work as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIYFviZCPd0w"
   },
   "outputs": [],
   "source": [
    "# Use ‚Äòuv pip install‚Äô to leverage uv‚Äôs parallel download/cache for much faster installs\n",
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUmZvGi-ozwR"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output  # for clearing cell output when done\n",
    "\n",
    "# Grouped by functionality:\n",
    "#  ‚Ä¢ Seaborn & UMAP             ‚Üí Plotting and dimensionality reduction\n",
    "#  ‚Ä¢ LiveLossPlot               ‚Üí Real-time training metric visualization\n",
    "#  ‚Ä¢ Accelerate & PEFT          ‚Üí Hardware acceleration and parameter-efficient fine-tuning\n",
    "#  ‚Ä¢ gensim, nltk               ‚Üí Topic modeling & NLP utilities\n",
    "#  ‚Ä¢ torchvision                ‚Üí Computer-vision datasets and transforms\n",
    "#  ‚Ä¢ ipywidgets                 ‚Üí Interactive notebook widgets\n",
    "#  ‚Ä¢ ipdb                       ‚Üí Interactive debugger\n",
    "#  ‚Ä¢ colorama                   ‚Üí Colored console output formatting\n",
    "#  ‚Ä¢ clear_output               ‚Üí Clears notebook cell output when install completes\n",
    "#  ‚Ä¢ Transformers & Datasets    ‚Üí NLP foundational libraries\n",
    "#  ‚Ä¢ Gemma==3                   ‚Üí Bundle of tokenizers & models (pinned to v3 for compatibility)\n",
    "\n",
    "!uv pip install  \\\n",
    "    seaborn \\\n",
    "    umap-learn \\\n",
    "    livelossplot \\\n",
    "    accelerate \\\n",
    "    peft \\\n",
    "    # gensim \\\n",
    "    nltk \\\n",
    "    # torchvision \\\n",
    "    datasets \\\n",
    "    # ipywidgets \\\n",
    "    ipdb \\\n",
    "    # colorama \\\n",
    "    tf-keras \\\n",
    "    transformers \\\n",
    "    huggingface_hub \\\n",
    "    # numpy==1.22.0\n",
    "\n",
    "# clear the long install output\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9-wAi7pZ3DI"
   },
   "outputs": [],
   "source": [
    "#gemma 3 does not work with uv install so it needs to be installed separately\n",
    "!pip install gemma==3\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4boGA9rYdt9l"
   },
   "outputs": [],
   "source": [
    "# Import system and math utilities\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import urllib.request\n",
    "import requests\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Check for connected accelerators (GPU or TPU) and set up accordingly\n",
    "if os.environ.get(\"COLAB_GPU\") and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
    "    print(\"A GPU is connected.\")\n",
    "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
    "    print(\"A TPU is connected.\")\n",
    "    import jax.tools.colab_tpu\n",
    "    jax.tools.colab_tpu.setup_tpu()\n",
    "else:\n",
    "    print(\"Only CPU accelerator is connected.\")\n",
    "\n",
    "# Avoid GPU memory allocation to be done by JAX\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
    "\n",
    "# Import libraries for JAX-based deep learning\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import optax\n",
    "\n",
    "# Import NLP and model-related libraries\n",
    "import transformers\n",
    "from transformers import  AutoTokenizer,  AutoModel\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering # For image processing.\n",
    "\n",
    "from gemma import gm\n",
    "\n",
    "# Import image processing and plotting libraries\n",
    "from livelossplot import PlotLosses\n",
    "import matplotlib.pyplot as plt\n",
    "import  numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Import additional utilities for working with text and models\n",
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Download an example image to use in the notebook\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
    "    \"cat.png\",\n",
    ")\n",
    "\n",
    "# Import libraries for NLP preprocessing and working with pre-trained models\n",
    "# import gensim\n",
    "from nltk.data import find\n",
    "import nltk\n",
    "nltk.download(\"word2vec_sample\")\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import Hugging Face tools and IPython widgets\n",
    "# import huggingface_hub\n",
    "# import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "# import colorama\n",
    "\n",
    "# Set Matplotlib to output SVG format for better quality plots\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-zS13TTVrPp"
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
    "print(f\"üíª CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9X10jhocGaS"
   },
   "outputs": [],
   "source": [
    "# @title [Run Me] Helper Plotting Functions.\n",
    "\n",
    "def plot_position_encodings(P, max_tokens, d_model):\n",
    "    \"\"\"\n",
    "    Plots the position encodings matrix.\n",
    "\n",
    "    Args:\n",
    "        P: Position encoding matrix (2D array).\n",
    "        max_tokens: Maximum number of tokens (rows) to plot.\n",
    "        d_model: Dimensionality of the model (columns) to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the plot size based on the number of tokens and model dimensions\n",
    "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
    "\n",
    "    # Plot the position encoding matrix with a color map for better visualization\n",
    "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
    "\n",
    "    # Add a color bar to indicate the encoding values\n",
    "    plt.colorbar(im, cmap=\"blue\")\n",
    "\n",
    "    # Show embedding indices as ticks if the dimensionality is small\n",
    "    if d_model <= 64:\n",
    "        plt.xticks(range(d_model))\n",
    "\n",
    "    # Show position indices as ticks if the number of tokens is small\n",
    "    if max_tokens <= 32:\n",
    "        plt.yticks(range(max_tokens))\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Embedding index\")\n",
    "    plt.ylabel(\"Position index\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
    "    \"\"\"\n",
    "    Plots an attention weight matrix with custom axis ticks.\n",
    "\n",
    "    Args:\n",
    "        weight_matrix: The attention weight matrix to plot.\n",
    "        x_ticks: Labels for the x-axis (typically the query tokens).\n",
    "        y_ticks: Labels for the y-axis (typically the key tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the plot size\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot the attention weight matrix as a heatmap\n",
    "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
    "\n",
    "    # Set custom ticks on the x and y axes\n",
    "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
    "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
    "\n",
    "    # Label the plot\n",
    "    plt.title(\"Attention matrix\")\n",
    "    plt.xlabel(\"Attention score\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check that the content is an actual image\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"image/\"):\n",
    "            raise ValueError(f\"Content at URL is not an image. Content-Type: {content_type}\")\n",
    "\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    except:\n",
    "        print(f\"Could not load image from {url}\\n \")\n",
    "        return None\n",
    "\n",
    "def resize_image(img, new_width=300):\n",
    "    w, h = img.size\n",
    "    new_height = int((new_width / w) * h)\n",
    "    return img.resize((new_width, new_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMkaKekB_pR4"
   },
   "outputs": [],
   "source": [
    "# @title [Run Me] Helper Text Processing Functions.\n",
    "\n",
    "def get_word2vec_embedding(words: list[str]):\n",
    "    \"\"\"\n",
    "    Fetch embeddings for a given list of words from a Word2Vec-style text file.\n",
    "\n",
    "    The file must start with a header line:\n",
    "        <vocab_size> <vector_dim>\n",
    "    followed by one word + its vector per line, e.g.:\n",
    "        fawn 0.0891758 0.121832 ‚Ä¶ 0.0872918\n",
    "\n",
    "    Args:\n",
    "        words: Iterable of tokens you want embeddings for.\n",
    "\n",
    "    Returns:\n",
    "        embeddings: jnp.ndarray of shape (n_found, vector_dim)\n",
    "        found_words: List[str] of the words (in the same order as embeddings).\n",
    "    \"\"\"\n",
    "    words_set = set(words)\n",
    "    found_embeddings = []\n",
    "    found_words = []\n",
    "    # Download from the Hub\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=\"AmelSellami/pruned-word2vec\",\n",
    "        filename=\"pruned.word2vec.txt\",\n",
    "        repo_type=\"dataset\",  \n",
    "    )\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Read & parse header\n",
    "        header = f.readline().strip().split()\n",
    "        if len(header) != 2:\n",
    "            raise ValueError(f\"Invalid header in {file_path!r}: {header}\")\n",
    "        vocab_size, dim = map(int, header)\n",
    "\n",
    "        # Scan each line for your target words\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            token = parts[0]\n",
    "            if token in words_set:\n",
    "                # parse floats; expect exactly `dim` numbers\n",
    "                vals = parts[1:]\n",
    "                if len(vals) != dim:\n",
    "                    raise ValueError(f\"Unexpected vector size for {token!r}: got {len(vals)} vs {dim}\")\n",
    "                vec = [float(x) for x in vals]\n",
    "                found_embeddings.append(vec)\n",
    "                found_words.append(token)\n",
    "                words_set.remove(token)\n",
    "                if not words_set:\n",
    "                    break  # got them all\n",
    "\n",
    "    embeddings = jnp.array(found_embeddings)\n",
    "    return embeddings, found_words\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Function that takes in a string and removes all punctuation.\"\"\"\n",
    "    import re\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def print_sample(prompt, sample, model_name=\"\", generation_time=None):\n",
    "\n",
    "    if prompt in sample:\n",
    "      sample = sample.split(prompt)[1].rstrip()\n",
    "    html = f\"\"\"\n",
    "    <div style=\"font-family:monospace; border:1px solid #ccc; padding:10px\">\n",
    "        <div><b style='color:teal;'>ü§ñ Model:</b> <span>{model_name}</span></div>\n",
    "        {'<div><b style=\"color:orange;\">‚è±Ô∏è Generation Time:</b> ' + f'{generation_time:.2f}s</div>' if generation_time else ''}\n",
    "        <div><b style='color:green;'>üìù Prompt:</b> {prompt}</div>\n",
    "        <div><b style='color:purple;'>‚ú® Generated:</b> {sample}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    Function that takes in a model name and returns the tokenizer for that model.\n",
    "    \"\"\"\n",
    "    if model_name == \"gemma3\":\n",
    "        tokenizer = gm.text.Gemma3Tokenizer()\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize(text: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Function that takes in a string and a tokenizer and returns the tokenized version of the string.\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = [tokenizer.decode(t) for t in token_ids]\n",
    "    if model_name != \"gemma3\":\n",
    "        tokens = [token.replace('ƒ†', ' ') for token in tokens] # Replace the 'ƒ†' prefix used by some tokenizers with a space\n",
    "    return tokens, token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElXJ1BGzH5BJ"
   },
   "source": [
    "## ü§ñ Charger un mod√®le depuis Hugging Face et interagir localement [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lrb9vL2uO5A"
   },
   "source": [
    "\n",
    "### üéØ Objectif\n",
    "\n",
    "* Apprendre √† **charger un mod√®le depuis Hugging Face** et ex√©cuter une inf√©rence √† l‚Äôaide d‚Äôun LLM\n",
    "\n",
    "* Charger un mod√®le l√©ger (par ex. gpt-neo-125m) et le solliciter avec une question simple\n",
    "\n",
    "* Exp√©rimenter avec diff√©rents param√®tres de g√©n√©ration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tADD6ZfEX-u"
   },
   "source": [
    "\n",
    "üìö Bienvenue dans le monde des grands mod√®les de langue!\n",
    "Nous sommes ravis de vous avoir √† bord!üéâ Avant de plonger dans la partie pratique de notre voyage, prenons un d√©tour rapide dans le monde fascinant de [Hugging Face](https://huggingface.co/) - une incroyable plateforme open source pour cr√©er et d√©ployer des mod√®les de langage √† la pointe de la technologie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me-nBsxtGBA1"
   },
   "source": [
    "##### üß† **Quels sont les grands mod√®les de langue?**\n",
    "\n",
    "Les mod√®les de grandes langues (LLMS) sont des syst√®mes d'IA form√©s sur de grandes quantit√©s de donn√©es de texte pour comprendre et g√©n√©rer du texte de type humain.Ils travaillent en apprenant des mod√®les dans la langue et en pr√©disant le mot le plus probable √©tant donn√© un certain contexte.\n",
    "\n",
    "**Concepts** cl√©s:\n",
    "\n",
    "*   Reconnaissance des mod√®les: les LLMS analysent des milliards de mots pour comprendre la langue\n",
    "*   Pr√©diction du mot de prochain: √Ä la base, ils devinent le mot suivant le plus probable\n",
    "*   Compr√©hension du contexte: ils consid√®rent l'ensemble de l'entr√©e lors de la r√©alisation des pr√©dictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltfz9jstGSTJ"
   },
   "source": [
    "#### **√Ä propos de Huggingface**\n",
    "\n",
    "<img src=\"https://www.hugging-face.org/wp-content/uploads/2023/11/hugging-faces.png\" alt=\"Alt Text\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s0Lc8rnGZt6"
   },
   "source": [
    "**Huggingface** est le \"Github de l'IA\" - une plate-forme qui d√©mocratise l'acc√®s aux mod√®les d'IA de pointe.Fond√©e en 2016, ils fournissent:\n",
    "\n",
    "\n",
    "* [Model Hub](https://huggingface.co/models): des milliers de mod√®les pr√©-form√©s pr√™ts √† l'emploi\n",
    "* [Biblioth√®que Transformers](https://huggingface.co/docs/transformateurs): outils faciles √† utiliser pour travailler avec des mod√®les de langue\n",
    "* [Ensembles de donn√©es](https://huggingface.co/datasets): ensembles de donn√©es organis√©s pour la formation et l'√©valuation\n",
    "* [Espaces](https://huggingface.co/spaces): plate-forme pour h√©berger des d√©mos et des applications ML\n",
    "\n",
    "\n",
    "Dans ce Colab, nous affichons les promt en <span style=\"color:green;\"><b>vert</b></span> et les √©chantillons g√©n√©r√©s par un mod√®le en <span style=\"color:purple;\"><b>violet</b></span>, comme dans l‚Äôexemple ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNt4n7hrGOY4"
   },
   "outputs": [],
   "source": [
    "print_sample(prompt='My fake prompt', sample=' is awesome!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1e-FfGzGrr_"
   },
   "source": [
    "##### **Votre premier mod√®le de langage **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8NCoWUnGwD9"
   },
   "source": [
    "**Plongeons** √† quel point il est simple de charger et d'interagir avec un mod√®le de l'Hugging Face!ü§ó\n",
    "\n",
    "Pour ce tutoriel, nous avons pr√©configur√© plusieurs options de mod√®le pour vous pour exp√©rimenter:\n",
    "\n",
    "* **ELEUTHERAI / GPT-NEO-125M** - Un mod√®le l√©ger avec 125 millions de param√®tres.C'est rapide et √©conome en m√©moire - grand pour commencer!\n",
    "* **GPT2 et GPT2-Medium** - Mod√®les classiques form√©s par OpenAI, avec des param√®tres de 117m et 355 m respectivement.La variante moyenne offre plus de ma√Ætrise et de coh√©rence.\n",
    "* **TIIUAE / FALCON-RW-1B** - Un mod√®le open source plus grand de la famille Falcon, avec 1 milliard de param√®tres.\n",
    "* **Microsoft / PHI-4** - Un mod√®le de pointe de Microsoft s'est concentr√© sur la g√©n√©ration de langage de haute qualit√© avec une empreinte m√©moire plus petite.\n",
    "\n",
    "Vous pouvez changer de mod√®le en red√©marrant le noyau Colab et en mettant √† jour la variable `model_name` dans la cellule ci-dessous.\n",
    "\n",
    "> üí° **Note :** Les √©tapes de chargement et d‚Äôinteraction pr√©sent√©es ici s‚Äôappliquent √† **tout** mod√®le Hugging Face qui prend en charge la g√©n√©ration de texte via l‚ÄôAPI `pipeline`. N‚Äôh√©sitez pas √† explorer au-del√† de cette liste !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c87TjRSRHCmg"
   },
   "source": [
    "Note : le mod√®le `microsoft/phi-4` peut prendre plus d‚Äôune demi-heure pour charger tous les fichiers n√©cessaires. Nous vous recommandons d‚Äôutiliser d‚Äôautres mod√®les pendant ce pratique et de vous familiariser avec Phi-4 plus tard, √† votre rythme.\n",
    "\n",
    "G√©n√©rons du texte :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvRivN3VG_yT"
   },
   "outputs": [],
   "source": [
    "# Set the model name to 'EleutherAI/gpt-neo-125M' (this can be changed via the dropdown options).\n",
    "model_name = 'gpt2'  # @param ['EleutherAI/gpt-neo-125M', 'gpt2', 'gpt2-medium', 'Qwen/Qwen3-0.6B', 'tiiuae/falcon-rw-1b','microsoft/phi-4']\n",
    "\n",
    "# Define the prompt for the text generation model.\n",
    "test_prompt = 'Once upon a time in a magical Kigali'  # @param {type: 'string'}\n",
    "\n",
    "# Create a text generation pipeline using the specified model.\n",
    "generator = transformers.pipeline('text-generation', model=model_name)\n",
    "\n",
    "# Generate text based on the provided prompt.\n",
    "# 'do_sample=True' enables sampling to introduce randomness in generation, and 'min_length=30' ensures at least 30 tokens are generated.\n",
    "model_output = generator(test_prompt, do_sample=True, min_length=30)\n",
    "\n",
    "clear_output() # Clear the output to keep the notebook tidy.\n",
    "\n",
    "# Print the generated text sample.\n",
    "print_sample(test_prompt, model_output[0]['generated_text'], model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiXSAfkdHJ0m"
   },
   "source": [
    "**üí°tip:** essayez d'ex√©cuter le code ci-dessus avec diff√©rentes invites ou avec la m√™me invite plus d'une fois!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxouM28THJXB"
   },
   "source": [
    "**ü§î** Discussion: Pourquoi pensez-vous que le texte g√©n√©r√© change √† chaque fois, m√™me avec la m√™me invite? √âcrivez votre r√©ponse dans le champ d'entr√©e ci-dessous et discutez avec votre voisin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMwaKFMnHUJg"
   },
   "source": [
    "<Dettots>\n",
    "<summary> <strong> R√©ponse </strong> </summary>\n",
    "\n",
    "Le mod√®le utilise l'√©chantillonnage avec le hasard (temp√©rature> 0) pour g√©n√©rer diverses sorties.\n",
    "M√™me avec la m√™me entr√©e, la nature probabiliste de la g√©n√©ration de texte conduit √† des r√©sultats diff√©rents.\n",
    "\n",
    "</fords>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nflrXCXHY6u"
   },
   "source": [
    "#### **Comprendre les param√®tres de g√©n√©ration**\n",
    "\n",
    "Les param√®tres de g√©n√©ration contr√¥lent comment le mod√®le produit du texte.Explorons les plus importants:\n",
    "\n",
    "### Temp√©rature\n",
    "\n",
    "Contr√¥le le caract√®re al√©atoire des pr√©dictions:\n",
    "\n",
    "- **Faible (0,1 √† 0,5):** sorties conservatrices et pr√©visibles\n",
    "\n",
    "- **Medium (0,6‚Äì1,0):** cr√©ativit√© et coh√©rence √©quilibr√©es\n",
    "\n",
    "- **√âlev√© (1,1‚Äì2,0):** tr√®s cr√©atif mais potentiellement incoh√©rent\n",
    "\n",
    "### TOP-P (√©chantillonnage du noyau)\n",
    "Contr√¥le la diversit√© en limitant le vocabulaire consid√©r√©:\n",
    "\n",
    "- **Faible (0,1 √† 0,3):** tr√®s concentr√© sur les mots les plus probables\n",
    "- **High (0.8‚Äì1.0):** Considers more word possibilities  \n",
    "\n",
    "\n",
    "Exp√©rimentons avec ces param√®tres:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cxq7nR0pHnJv"
   },
   "outputs": [],
   "source": [
    "# @title Choose Model and Prompt { run: \"auto\" }\n",
    "model_name = \"gpt2-medium\"  # @param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
    "prompt = \"Once upon a time in a magical Kigali,\"  # @param {type:\"string\"}\n",
    "temperature = 1  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "top_p = 0.2  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "max_new_tokens = 64  # @param {type:\"slider\", min:10, max:256, step:1}\n",
    "seed = 2  # @param {type:\"integer\"}\n",
    "\n",
    "\n",
    "def run_sample(\n",
    "    model_name,  # The language model we‚Äôll use to generate text\n",
    "    prompt: str,  # The text prompt we'll give to the model to start the text generation\n",
    "    seed: int | None = None,  # Optional: A number to make the results predictable each time\n",
    "    temperature: float = 0.6,  # Controls how random the model‚Äôs output is; lower values make it more focused\n",
    "    top_p: float = 0.9,  # Controls how much of the most likely words are considered; higher values consider more options\n",
    "    max_new_tokens: int = 64,  # The maximum number of words or tokens the model will add to the prompt\n",
    ") -> str:\n",
    "    # This function generates text based on a given prompt using a language model,\n",
    "    # with options to control randomness, the number of tokens generated, and reproducibility.\n",
    "\n",
    "    # Load the model based on selection\n",
    "    if 'gpt2' in model_name:\n",
    "        tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
    "        model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{model_name} is not yet supported.\")\n",
    "\n",
    "    clear_output()\n",
    "    # Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    # Align tokenizer padding\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Convert the prompt text into tokens that the model can process\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Extract the tokens (input IDs) and attention mask (to focus on important parts) from the inputs\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Move the tokens and attention mask to the same device as the model (like a GPU if available)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    # Set up how we want the model to generate text\n",
    "    generation_config = transformers.GenerationConfig(\n",
    "        do_sample=True,  # Allow the model to add some randomness to its text generation\n",
    "        temperature=temperature,  # Adjust how random the output is; lower means more focused\n",
    "        top_p=top_p,  # Consider the most likely words that make up the top 90% of possibilities\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Use the token ID that represents padding (extra space)\n",
    "        top_k=0,  # We're not limiting to the top-k words, so we set this to 0\n",
    "    )\n",
    "\n",
    "    # If a seed is provided, set it so that the results are repeatable (same output each time)\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Generate text using the model with the settings we defined\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,  # Provide the input tokens to the model\n",
    "        attention_mask=attention_mask,  # Provide the attention mask to help the model focus\n",
    "        return_dict_in_generate=True,  # Ask the model to return detailed information\n",
    "        output_scores=True,  # Include the scores (confidence levels) for the generated tokens\n",
    "        max_new_tokens=max_new_tokens,  # Set the maximum number of tokens to generate\n",
    "        generation_config=generation_config,  # Apply our custom text generation settings\n",
    "    )\n",
    "\n",
    "    # Make sure only one sequence (output) is generated, to keep things simple\n",
    "    assert len(generation_output.sequences) == 1\n",
    "\n",
    "    # Get the generated sequence of tokens\n",
    "    output_sequence = generation_output.sequences[0]\n",
    "\n",
    "    # Convert the generated tokens back into readable text\n",
    "    output_string = tokenizer.decode(output_sequence)\n",
    "\n",
    "    # Print the prompt and the generated response\n",
    "    print_sample(prompt, output_string, model_name=model_name)\n",
    "\n",
    "    # Return the generated text response\n",
    "    return output_string\n",
    "\n",
    "# Run the interactive generation\n",
    "_ = run_sample(\n",
    "    model_name=model_name,\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    seed=seed,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "filsIJLTIEkv"
   },
   "source": [
    "\n",
    "üéØ **Essayez ceci :** Exp√©rimentez avec diff√©rents prompts et valeurs de temp√©rature. Quels motifs remarquez-vous ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5U3YbK_IGel"
   },
   "source": [
    "#### **Mod√®les linguistiques dans les applications du monde r√©el**\n",
    "\n",
    "Les mod√®les linguistiques ont de nombreuses applications pratiques.Explorons quelques-uns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQxfHAkuIZCM"
   },
   "source": [
    "**G√©n√©ration** de code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdnWggRcIXwo"
   },
   "outputs": [],
   "source": [
    "code_prompt = 'Write a Python function that calculates the fibonacci sequence:' # @param {type:'string'}\n",
    "model_name = 'gpt2'  # @param ['gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M']\n",
    "code_result = run_sample(model_name, code_prompt, temperature=0.3, max_new_tokens=200)\n",
    "print('üíª Code Generation:')\n",
    "print(code_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PbWOseNIr6l"
   },
   "source": [
    "**Question** R√©pondre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07daslXuIpgI"
   },
   "outputs": [],
   "source": [
    "qa_prompt = 'What are the main advantages of using version control in software development?' # @param {type:'string'}\n",
    "model_name = 'gpt2'  # @param ['gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M']\n",
    "qa_result = run_sample(model_name, qa_prompt, temperature=0.5, max_new_tokens=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDThwJmWIyCR"
   },
   "source": [
    "**√âcriture** cr√©ative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmEK-JD5IzMf"
   },
   "outputs": [],
   "source": [
    "story_prompt = 'Write the opening paragraph of a science fiction story:' # @param {type:'string'}\n",
    "model_name = 'EleutherAI/gpt-neo-125M'  # @param ['gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M']\n",
    "story_result = run_sample(model_name, story_prompt, temperature=0.9, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7bDaHeKa0kN"
   },
   "source": [
    "**Question** de vision R√©pondre\n",
    "\n",
    "La r√©ponse √† la question visuelle (VQA) est une t√¢che d'IA multimodale qui combine la vision informatique et la compr√©hension du langage naturel.L'objectif est simple mais puissant: √©tant donn√© une image et une question de langue naturelle √† ce sujet, le mod√®le doit g√©n√©rer une r√©ponse pertinente et pr√©cise.\n",
    "\n",
    "Dans l'exemple ci-dessous, nous utiliserons un mod√®le pr√©-form√© de HuggingFace pour montrer comment VQA fonctionne dans la pratique.\n",
    "\n",
    "> Ex√©cutez √† nouveau la cellule si vous rencontrez une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v7yJn-Ta74X"
   },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Lake_Naivasha%2C_Kenya_%2832487531978%29.jpg/2880px-Lake_Naivasha%2C_Kenya_%2832487531978%29.jpg\" # @param {type:'string'}\n",
    "image = load_image_from_url(image_url)\n",
    "display(resize_image(image, new_width=800))\n",
    "\n",
    "# Ask a question.\n",
    "question = \"What is in the picture?\" # @param {type:'string'}\n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"Salesforce/blip-vqa-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Prepare inputs.\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference.\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the answer.\n",
    "answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "clear_output()\n",
    "display(resize_image(image, new_width=800))\n",
    "print('')\n",
    "print_sample(question, answer, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXrCIqpc6PQk"
   },
   "source": [
    "**Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dy702C0vxRI1"
   },
   "outputs": [],
   "source": [
    "model = gm.nn.Gemma3_1B()\n",
    "params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_1B_IT)\n",
    "\n",
    "sampler = gm.text.ChatSampler(\n",
    "    model=model,\n",
    "    params=params,\n",
    "    multi_turn=True,\n",
    ")\n",
    "\n",
    "user = 'Share one metaphor linking \"shadow\" and \"laughter\".' # @param {type:'string'}\n",
    "\n",
    "turn0 = sampler.chat(user)\n",
    "print_sample(user, turn0, model_name=\"Gemma3_1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Rb8_6hP6xa4"
   },
   "outputs": [],
   "source": [
    "user = 'Expand it in a haiku.' # @param {type:'string'}\n",
    "turn1 = sampler.chat(user)\n",
    "print_sample(user, turn1, model_name=\"Gemma3_1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEpNUKkoJOUj"
   },
   "source": [
    "\n",
    "Plut√¥t cool, non ? ü§©\n",
    "Aujourd‚Äôhui, nous allons aller un peu plus loin ‚Äî en **entra√Ænant notre propre LLM inspir√© de Shakespeare** ! Cette exp√©rience pratique nous aidera √† comprendre comment ces mod√®les fonctionnent r√©ellement **dans les coulisses**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svfeQO7VIOIu"
   },
   "source": [
    "## üîç Architecture du transformateur R√©capitulatif rapide [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brwKNHT0-Hhl"
   },
   "source": [
    "\n",
    "L'architecture du transformateur a √©t√© introduite dans l'article intitul√© [l'attention est tout ce dont vous avez besoin] (https://proekedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-paper.pdf) par Vaswani et al.Comme le titre de l'article le sugg√®re, une telle architecture se compose essentiellement de m√©canismes d'attention ainsi que des couches d'alimentation et des couches lin√©aires, comme le montre le diagramme ci-dessous.\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
    "\n",
    "Les transformateurs et ses variations sont au c≈ìur des mod√®les de grande langue et ce n'est pas une exag√©ration de dire que presque tous les mod√®les de langue sont des architectures bas√©es sur les transformateurs.Comme vous pouvez le voir dans le diagramme, l'architecture du transformateur d'origine se compose de deux parties, une qui re√ßoit des entr√©es g√©n√©ralement appel√©es encodeur et une autre qui re√ßoit des sorties (c'est-√†-dire des cibles) appel√©e d√©codeur.En effet, le transformateur a √©t√© con√ßu pour la traduction automatique.\n",
    "\n",
    "Dans ce tutoriel, nous nous concentrerons uniquement sur la partie d√©codeur qui est l'architecture qui alimente les mod√®les de grande langue les plus modernes comme Chatgpt.\n",
    "\n",
    "### Pr√©sentation du d√©codeur du transformateur\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/$s_!qbpc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png\" width=\"650\" height=\"400\" />\n",
    "\n",
    "\n",
    "* **Pr√©paration de l‚Äôentr√©e**\n",
    "\n",
    "  * Texte brut ‚Üí **Tokenizer** ‚Üí s√©quence d‚Äôidentifiants de tokens\n",
    "  * Identifiants ‚Üí **Repr√©sentations des tokens** + **Repr√©sentations positionnelles** ‚Üí vecteurs d‚Äôentr√©e\n",
    "\n",
    "* **Blocs d√©codeurs empil√©s** (r√©p√©t√©s *N* fois)\n",
    "\n",
    "  1. **Normalisation de couche**\n",
    "  2. **Auto-attention multi-t√™te masqu√©e** (masque causal)\n",
    "\n",
    "     * La connexion r√©siduelle ajoute la sortie de l‚Äôattention √† son entr√©e\n",
    "  3. **Normalisation de couche**\n",
    "  4. **R√©seau feed-forward** (MLP)\n",
    "\n",
    "     * Deux couches lin√©aires + non-lin√©arit√©, appliqu√©es position par position\n",
    "     * La connexion r√©siduelle ajoute la sortie du FFN √† son entr√©e\n",
    "\n",
    "* **Projection de sortie**\n",
    "\n",
    "  * Vecteurs finaux du d√©codeur ‚Üí couche lin√©aire ‚Üí logits du vocabulaire ‚Üí softmax pour les probabilit√©s du token suivant\n",
    "\n",
    "Commen√ßons notre parcours en comprenant comment les mod√®les tokenisent le texte.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtWMaTddww65"
   },
   "source": [
    "## üß± Tokenisation [<font color = 'orange'> d√©butant </font>]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WO9OWhQYMti4"
   },
   "source": [
    "Les mod√®les de langage naturel fonctionnent sur des entr√©es num√©riques discr√®tes, alors que le texte brut est une s√©quence de caract√®res. La **tokenisation** est le pont entre le texte lisible par l‚Äôhumain et les vecteurs d‚Äôentr√©e exploitables par le mod√®le. De mani√®re g√©n√©rale, la tokenisation :\n",
    "\n",
    "1. **Divise le texte en unit√©s (¬´ tokens ¬ª)**\n",
    "\n",
    "   * S√©pare une cha√Æne de caract√®res en mots, sous-mots ou caract√®res.\n",
    "2. **Associe chaque token √† un identifiant entier**\n",
    "\n",
    "   * Utilise un vocabulaire fixe pour que chaque token corresponde √† un indice unique.\n",
    "3. **Permet le traitement par lots et la recherche d‚Äôembeddings**\n",
    "\n",
    "   * Convertit des textes de longueur variable en s√©quences d‚Äôidentifiants remplies (padded) pouvant √™tre trait√©es par des r√©seaux neuronaux.\n",
    "\n",
    "Un token peut √™tre :\n",
    "\n",
    "* Un seul caract√®re (`i`, `n`, `d`, `a`, `b`)\n",
    "* Un sous-mot (`ind`, `aba`)\n",
    "* Un mot entier (`indaba`)\n",
    "\n",
    "Un vocabulaire est la liste fixe des tokens (mots, sous-mots ou caract√®res) qu‚Äôun mod√®le conna√Æt, chacun √©tant associ√© √† un identifiant entier unique pour la recherche d‚Äôembeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7uolj800B4z"
   },
   "source": [
    "\n",
    "\n",
    "Diff√©rents mod√®les ‚Äî comme GPT, Gemma, LLaMA, Mistral, et d‚Äôautres ‚Äî utilisent des tokenizers diff√©rents, chacun prenant ses propres d√©cisions sur la mani√®re de d√©couper le texte en tokens. La m√©thode de tokenisation la plus courante dans les LLM est le **Byte Pair Encoding (BPE)**. Si vous √™tes curieux de savoir comment cela fonctionne, cette [excellente vid√©o](https://www.youtube.com/watch?v=zduSFxRajkE) l‚Äôexplique tr√®s bien.\n",
    "\n",
    "> L‚Äôid√©e cl√© derri√®re la tokenisation est la **granularit√©** ‚Äî √† quel point un mod√®le doit-il d√©couper le texte pour comprendre et pr√©dire ce qui vient ensuite ? L‚Äôobjectif est de trouver un √©quilibre : d√©couper le texte en morceaux suffisamment petits pour que le mod√®le puisse bien g√©n√©raliser, mais pas trop petits au risque d‚Äôexploser le nombre de tokens. Un bon tokenizer garde un vocabulaire compact, g√®re efficacement la diversit√© des langues et compresse bien le texte, de sorte que moins de tokens sont n√©cessaires pour repr√©senter le sens ‚Äî surtout dans des contextes multilingues.\n",
    "\n",
    "La taille du vocabulaire correspond au nombre de tokens distincts (mots, sous-mots ou symboles) reconnus par le tokenizer d‚Äôun mod√®le.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ku_PEI0PF4l"
   },
   "source": [
    "### **üéØ Essayez vous-m√™me :** Playground du Tokenizer\n",
    "\n",
    "Passons √† la pratique. Visitez l‚Äôun des sites suivants :\n",
    "\n",
    "* [Tiktokenizer Playground (GPT-2)](https://tiktokenizer.vercel.app/?model=gpt2)\n",
    "* [OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
    "\n",
    "Collez la phrase :\n",
    "`Welcome to the Indaba LLM tutorial happening in Kigali. Get ready to explore the world of LLMs.`\n",
    "\n",
    "<!-- üéØ Maintenant, essayez la m√™me phrase dans une autre langue que vous parlez ‚Äî yor√πb√°, kiswahili, fran√ßais, etc. Notez ce que vous avez observ√©. -->  \n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1XpIVAOk281R7i13IMYQHe0HZZG6tUrjw\" alt=\"TikTokenizer\" width=\"800\"/>\n",
    "  <figcaption><em></em></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6qNp4IhGP-K"
   },
   "source": [
    "### Jouez avec Gemma Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_Ao6l4ZGWyS"
   },
   "outputs": [],
   "source": [
    "tokenizer = gm.text.Gemma3Tokenizer()\n",
    "tokenized_prompt = tokenizer.encode('Glad to be at the Indaba!', add_bos=True)\n",
    "tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCir3oSyGhIi"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([\n",
    " 122637,\n",
    " 531,\n",
    " 577,\n",
    " 657,\n",
    " 506,\n",
    " 1851,\n",
    " 6525,\n",
    " 236888,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFa34_iaIVwx"
   },
   "outputs": [],
   "source": [
    "#Let's tokenize a different language\n",
    "arabic_tokens=tokenizer.encode('ÿ•ŸÜŸá ŸäŸàŸÖ ÿ¨ŸÖŸäŸÑ ÿßŸÑŸäŸàŸÖ')\n",
    "arabic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdOcWp9CInOn"
   },
   "outputs": [],
   "source": [
    "print('Arabic prompt got tokenized into the following tokens: \\n',\n",
    "      tokenizer.decode(arabic_tokens[0]), \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[1]),  \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[2]),  \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[3]),  \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[4]),  \"\\n\",\n",
    "\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YPYutB1TAes"
   },
   "source": [
    "## ìä≥ Embeddings [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-4wEi2kVz3v"
   },
   "source": [
    "Apr√®s la tokenisation, chaque ID de token est associ√© √† un vecteur dense via la **couche d‚Äôembedding**. Ces embeddings capturent l‚Äôinformation s√©mantique des tokens et servent d‚Äôentr√©e pour le reste du mod√®le.\n",
    "\n",
    "**Embeddings de tokens**\n",
    "Une table de correspondance apprise de forme `(taille_du_vocabulaire √ó d_model)`. Chaque ID de token devient un vecteur de dimension `d_model`.\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1mReprFfL9ezlIRh55Co0yzX3EjiwcHsf\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
    "<Figcaption> <em> </em> </gigcaption>\n",
    "</ figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A7eVrS1bCCz"
   },
   "source": [
    "Dans cette section, nous allons extraire directement les embeddings de tokens appris par le mod√®le Word2Vec pour un petit ensemble de mots exemples, puis utiliser l'analyse en composantes principales (ACP) pour projeter ces vecteurs de haute dimension en deux dimensions (2D). Enfin, nous tracerons les coordonn√©es en 2D afin de visualiser comment les tokens s√©mantiquement li√©s se regroupent naturellement dans l‚Äôespace d‚Äôembedding.\n",
    "\n",
    "L‚ÄôACP est une m√©thode de r√©duction de dimension qui pr√©serve les relations locales ‚Äî ainsi, dans le graphique obtenu, vous devriez voir des tokens similaires (comme ¬´ chien ¬ª vs ¬´ chat ¬ª ou ¬´ roi ¬ª vs ¬´ reine ¬ª) regroup√©s √† proximit√©.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDT-Faavc4sb"
   },
   "outputs": [],
   "source": [
    "#  1. A set of tokens\n",
    "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
    "word_embeddings, words = get_word2vec_embedding(words)\n",
    "\n",
    "# # 4. Apply PCA to reduce dimensionality\n",
    "# `n_components=2` reduces the n-dimensional vector to 2 dimensions i.e 2 columns\n",
    "# but preserving the local relationships.\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca.fit_transform(word_embeddings)\n",
    "\n",
    "# 5. Visualize the 2D embeddings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0)\n",
    "\n",
    "# Add annotations (the words) to each point\n",
    "for i, txt in enumerate(words):\n",
    "    ax.annotate(txt, (X_2d[i, 0], X_2d[i, 1]),\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=12,\n",
    "                fontweight='medium')\n",
    "\n",
    "plt.title('PCA Visualization of Word Embeddings from Word2Vec', fontsize=16)\n",
    "plt.xlabel('PCA  Component 1', fontsize=12)\n",
    "plt.ylabel('PCA Component 2', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxleHRaRJ6q7"
   },
   "source": [
    "L‚Äôimage APC ci-dessus vous apprend deux choses sur les embeddings :\n",
    "\n",
    "1. Similarit√© s√©mantique = proximit√© g√©om√©trique. Les mots ayant des significations ou contextes d‚Äôutilisation similaires se retrouvent proches les uns des autres.\n",
    "\n",
    "2. Analogies lin√©aires. Bien que non illustr√© ici, des d√©calages vectoriels tels que roi - homme ‚âà reine - femme sont possibles.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNO703V9SBcI"
   },
   "source": [
    "## ‚ïë Encodages de position: pourquoi l'ordre compte [<font color = 'orange'> d√©butant </font>]\n",
    "<! - (10 minutes) ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZIytxyvAqd9"
   },
   "source": [
    "\n",
    "**Pourquoi les embeddings positionnels ?**\n",
    "\n",
    "* Les embeddings de tokens seuls sont invariants par permutation, ce qui signifie qu‚Äôils ne savent pas quel token est venu en premier..\n",
    "* L‚Äôordre des mots est crucial pour le sens (¬´ Je suis heureux ¬ª ‚â† ¬´ Suis-je heureux ¬ª).\n",
    "\n",
    "**Comment fonctionnent les embeddings positionnels :**\n",
    "\n",
    "1. **Encodages fixes (sinuso√Ødaux)**\n",
    "\n",
    "   * Fonctions pr√©-calcul√©es de la position (sinus et cosinus √† diff√©rentes fr√©quences).\n",
    "   * Pas de param√®tres suppl√©mentaires ; supporte des s√©quences de longueur arbitraire.\n",
    "2. **Embeddings positionnels appris**\n",
    "\n",
    "   * Table de correspondance entra√Ænable de forme `(longueur_max_s√©quence √ó d_model)`.\n",
    "   * Chaque position a son propre vecteur d‚Äôembedding appris pendant l‚Äôentra√Ænement.\n",
    "\n",
    "**Combinaison token + position :**\n",
    "\n",
    "```text\n",
    "final_embedding[i] = token_embedding[i] + pos_embedding[i]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxkDif_aRGKy"
   },
   "source": [
    "##### **Fonctions sinus et cosinus: un moyen simple d'ajouter des informations de position**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJIyboPkRG-o"
   },
   "source": [
    "Pour r√©pondre aux propri√©t√©s souhait√©es √©voqu√©es ci-dessus, les auteurs de [*Attention is All You Need*](https://arxiv.org/pdf/1706.03762) proposent une technique simple d‚Äô**encodage positionnel**. Cette m√©thode injecte l‚Äôinformation sur l‚Äôordre des tokens dans les embeddings en appliquant une combinaison de fonctions sinus et cosinus √† diff√©rentes fr√©quences.\n",
    "\n",
    "L‚Äôencodage positionnel pour une position donn√©e `pos`, √† l‚Äôindice de dimension d‚Äôembedding `i`, avec une taille totale d‚Äôembedding `d_model`, est d√©fini par :\n",
    "\n",
    "$$\n",
    "PE_{\\text{pos}, i} =\n",
    "\\begin{cases}\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{i / d_{\\text{model}}}}\\right), & \\text{si } i \\bmod 2 = 0 \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{(i - 1) / d_{\\text{model}}}}\\right), & \\text{si } i \\bmod 2 = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "En supposant un mod√®le avec une taille d‚Äôembedding $d_{\\text{model}} = 8$, le vecteur d‚Äôencodage positionnel pour la position `pos` devient :\n",
    "\n",
    "$$\n",
    "PE_{\\text{pos}} =\n",
    "\\begin{bmatrix}\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{0 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{0 / 8}}\\right) \\\\\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{2 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{2 / 8}}\\right) \\\\\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{4 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{4 / 8}}\\right) \\\\\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{6 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{6 / 8}}\\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **Note :** Les indices pairs utilisent le sinus, les indices impairs le cosinus. La division par des puissances de 10000 permet √† chaque dimension d‚Äôencoder une fr√©quence diff√©rente.\n",
    "\n",
    "Pour comprendre pourquoi ces encodages fonctionnent en pratique, cr√©ons une fonction pour les visualiser et exp√©rimenter avec la `token_sequence_length` et la dimension de l‚Äô`embedding` des tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQVRmEJKRLMq"
   },
   "outputs": [],
   "source": [
    "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
    "\n",
    "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
    "\n",
    "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
    "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
    "\n",
    "  i = jnp.arange(0, token_embedding, 2)\n",
    "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
    "  frequencies = positions * frequency_steps\n",
    "\n",
    "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
    "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
    "\n",
    "  return P\n",
    "\n",
    "token_sequence_length = 50 # @param {type: \"number\"}\n",
    "token_embedding = 768  # @param {type: \"number\"}\n",
    "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
    "plot_position_encodings(P, token_sequence_length, token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSxfPQhoRHBM"
   },
   "source": [
    "Remarquez comment chaque ligne du graphique, correspondant √† une position pr√©cise dans la s√©quence, affiche un motif ondul√© distinct √† travers les dimensions de l‚Äôembedding. Cela signifie que chaque position poss√®de un encodage fixe et unique, ce qui permet au mod√®le de diff√©rencier les tokens selon leur position dans la s√©quence. Ces encodages ne changent pas d‚Äôune ex√©cution √† l‚Äôautre ; ils sont enti√®rement d√©termin√©s par la formule.\n",
    "\n",
    "ü§î **Activit√© de groupe :**\n",
    "\n",
    "* <font color='orange'>Prenez un moment avec votre voisin pour explorer pourquoi ce motif sp√©cifique appara√Æt lorsque `token_sequence_length` est r√©gl√© √† 1000 et que `token_embedding` vaut 768.</font>\n",
    "* <font color='orange'>Exp√©rimentez avec des valeurs plus petites pour `token_sequence_length` et `token_embedding` afin de mieux comprendre et enrichir votre discussion.</font>\n",
    "* <font color='orange'>Vous vous demandez pourquoi la constante 10000 est utilis√©e ? Demandez √† votre voisin ce qu‚Äôil en pense.</font>\n",
    "* <font color='orange'>Essayez maintenant de r√©gler `token_sequence_length` √† 50 et `token_embedding` √† une valeur beaucoup plus grande, comme 10000. Qu‚Äôobservez-vous ? Avons-nous toujours besoin d‚Äôun embedding de token aussi grand ?</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qkh0KgRPdDf8"
   },
   "source": [
    "## Les embeddings informent l‚Äôattention [<font color = 'orange'> d√©butant </font>]\n",
    "\n",
    "<! - (10 minutes) ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo90Dfyrb9Kf"
   },
   "source": [
    "Comme nous l‚Äôavons appris ci-dessus, les embeddings projettent chaque token dans un espace vectoriel continu o√π les relations s√©mantiques se traduisent par une proximit√© g√©om√©trique. Les m√©canismes d‚Äôattention s‚Äôappuient directement sur ces embeddings en calculant des scores de similarit√© entre les vecteurs de tokens pour d√©terminer √† quel point chaque token doit ¬´ pr√™ter attention ¬ª aux autres tokens de la s√©quence. En d‚Äôautres termes, les embeddings fournissent les caract√©ristiques brutes, et l‚Äôattention utilise ces caract√©ristiques pour pond√©rer et combiner dynamiquement l‚Äôinformation entre les tokens.\n",
    "\n",
    "Ci-dessous, vous √©crirez une fonction qui impl√©mente l‚Äôattention par produit scalaire. L‚Äôobjectif est de calculer un vecteur de contexte, $c_t$, qui r√©sume l‚Äôinformation contenue dans les √©tats cach√©s ($H$) pertinente pour l‚Äô√©tat pr√©c√©dent ($q$).\n",
    "\n",
    "Cela se fait en trois √©tapes :\n",
    "\n",
    "* Calculer les scores d‚Äôattention (S) : calculer le produit scalaire du vecteur requ√™te $q$ avec tous les vecteurs cl√©s dans $H$. Cela donne la similarit√© entre chaque paire de vecteurs. Pour simplifier, nous consid√©rerons que l‚Äôembedding de chaque token sert √† la fois de cl√© et de requ√™te.\n",
    "\n",
    "* Calculer les poids d‚Äôattention ($\\alpha$) : appliquer une fonction softmax aux scores pour les normaliser en une distribution de probabilit√©.\n",
    "\n",
    "* Calculer le vecteur contexte ($c_t$) : calculer la somme pond√©r√©e des vecteurs valeurs (ici, $H$) en utilisant les poids d‚Äôattention.\n",
    "\n",
    "Ces √©tapes sont r√©sum√©es par les √©quations suivantes :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S &= q \\cdot H^T \\\\\n",
    "\\alpha &= \\text{softmax}(S) \\\\\n",
    "c_t &= \\alpha \\cdot H\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Enfin, nous visualiserons les poids d‚Äôattention r√©sultants pour un petit ensemble de mots exemples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-xcLmAFLl6G"
   },
   "outputs": [],
   "source": [
    "def dot_product_attention(hidden_states, previous_state):\n",
    "    \"\"\"\n",
    "    Calculate the dot product between the hidden states and previous states.\n",
    "\n",
    "    Args:\n",
    "        hidden_states: A tensor with shape [T_hidden, dm]\n",
    "        previous_state: A tensor with shape [T_previous, dm]\n",
    "    \"\"\"\n",
    "\n",
    "    # Hint: To calculate the attention scores, think about how you can use the `previous_state` vector\n",
    "    # and the `hidden_states` matrix. You want to find out how much each element in `previous_state`\n",
    "    # should \"pay attention\" to each element in `hidden_states`. Remember that in matrix multiplication,\n",
    "    # you can find the relationship between two sets of vectors by multiplying one by the transpose of the other.\n",
    "    # Hint: Use `jnp.matmul` to perform the matrix multiplication between `previous_state` and the\n",
    "    # transpose of `hidden_states` (`hidden_states.T`).\n",
    "    scores = ...  # FINISH ME\n",
    "\n",
    "    # Hint: Now that you have the scores, you need to convert them into probabilities.\n",
    "    # A softmax function is typically used in attention mechanisms to turn raw scores into probabilities\n",
    "    # that sum to 1. This will help in determining how much focus should be placed on each hidden state.\n",
    "    # Hint: Use `jax.nn.softmax` to apply the softmax function to `scores`.\n",
    "    w_n = ...  # FINISH ME\n",
    "\n",
    "    # Multiply the weights by the hidden states to get the context vector\n",
    "    # Hint: Use `jnp.matmul` again to multiply the attention weights `w_n` by `hidden_states`\n",
    "    # to get the context vector.\n",
    "    c_t = jnp.matmul(w_n, hidden_states)\n",
    "\n",
    "    return w_n, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFDyXq77Ll8c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title Run me to test your code\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "x = jax.random.normal(key, [2, 2])\n",
    "\n",
    "try:\n",
    "  w_n, c_t = dot_product_attention(x, x)\n",
    "\n",
    "  w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
    "  c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
    "  assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
    "  assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
    "\n",
    "  print(\"It seems correct. Look at the answer below to compare methods.\")\n",
    "except:\n",
    "  print(\"It looks like the function isn't fully implemented yet. Try modifying it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqBReNvBLl_M"
   },
   "outputs": [],
   "source": [
    "\n",
    "# when changing these words, note that if the word is not in the original\n",
    "# training corpus it will not be shown in the weight matrix plot.\n",
    "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
    "def dot_product_attention(hidden_states, previous_state):\n",
    "    # Calculate the attention scores:\n",
    "    # Multiply the previous state vector by the transpose of the hidden states matrix.\n",
    "    # This gives us a matrix of scores that show how much attention each element in the previous state\n",
    "    # should pay to each element in the hidden states.\n",
    "    # The result is a matrix of shape [T, N], where:\n",
    "    # T is the number of elements in the hidden states,\n",
    "    # N is the number of elements in the previous state.\n",
    "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
    "\n",
    "    # Apply the softmax function to the scores to convert them into probabilities.\n",
    "    # This normalizes the scores so that they sum up to 1 for each element,\n",
    "    # allowing us to interpret them as how much attention should be given to each hidden state.\n",
    "    w_n = jax.nn.softmax(scores)\n",
    "\n",
    "    # Calculate the context vector (c_t):\n",
    "    # Multiply the attention weights (w_n) by the hidden states.\n",
    "    # This combines the hidden states based on how much attention each one deserves,\n",
    "    # resulting in a new vector that represents the weighted sum of the hidden states.\n",
    "    # The resulting shape is [T, d], where:\n",
    "    # T is the number of elements in the previous state,\n",
    "    # d is the dimension of the hidden states.\n",
    "    c_t = jnp.matmul(w_n, hidden_states)\n",
    "\n",
    "    # Return the attention weights and the context vector.\n",
    "    return w_n, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N83teuL2CkmN"
   },
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
    "word_embeddings, words = get_word2vec_embedding(words)\n",
    "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
    "plot_attention_weight_matrix(weights, words, words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXQaSJMGELFC"
   },
   "source": [
    "En regardant la matrice, on peut voir quels mots ont des significations similaires. Le groupe de mots ¬´ royal ¬ª pr√©sente des scores d‚Äôattention plus √©lev√©s entre eux que les mots du groupe ¬´ nourriture ¬ª, qui eux aussi s‚Äôattachent les uns aux autres. On observe √©galement que le mot ¬´ ordinateur ¬ª obtient des scores d‚Äôattention tr√®s faibles avec tous les autres, ce qui montre qu‚Äôil est peu li√© aux mots des groupes ¬´ royal ¬ª ou ¬´ nourriture ¬ª.\n",
    "\n",
    "Note : Le produit scalaire est seulement l‚Äôune des mani√®res d‚Äôimpl√©menter la fonction de score dans les m√©canismes d‚Äôattention. Vous trouverez une liste plus compl√®te dans cet [article de blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) de Dr Lilian Weng.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY02IFQouwjN"
   },
   "source": [
    "## üîç ATTENTION [<FONT COLOR = 'GREEN'> Interm√©diaire </font>]\n",
    "\n",
    "<! - (25 minutes) ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qJdLHPBL1I8"
   },
   "source": [
    "### Entre l‚Äôauto-attention et l‚Äôattention multi-t√™te\n",
    "\n",
    "L‚Äôauto-attention et l‚Äôattention multi-t√™te (MHA) sont des composants fondamentaux de l‚Äôarchitecture des transformeurs. Dans cette section, nous expliquerons en d√©tail l‚Äôintuition derri√®re ces concepts ainsi que leur mise en ≈ìuvre. Plus tard, dans la section Transformeurs, vous apprendrez comment ces m√©canismes d‚Äôattention sont utilis√©s pour cr√©er un mod√®le s√©quence-√†-s√©quence reposant enti√®rement sur l‚Äôattention.\n",
    "\n",
    "Au fur et √† mesure de notre progression, nous repr√©senterons les phrases en les d√©composant en mots individuels et en encodant chaque mot √† l‚Äôaide du mod√®le word2vec pr√©sent√© pr√©c√©demment. Dans la section Transformeurs, nous √©tudierons plus en d√©tail comment les s√©quences d‚Äôentr√©e sont transform√©es en une s√©rie de vecteurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfEhWwBaLmFK"
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Embed a sentence using word2vec; for example use cases only.\n",
    "    \"\"\"\n",
    "    # clean sentence (not necessary if using a proper LLM tokenizer)\n",
    "    sentence = remove_punctuation(sentence)\n",
    "\n",
    "    # extract individual words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # get the word2vec embedding for each word in the sentence\n",
    "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
    "\n",
    "    # return with extra dimension (useful for creating batches later)\n",
    "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUPfggF9L9tE"
   },
   "source": [
    "### Auto-attention\n",
    "\n",
    "Une question simple √† propos de cette phrase est : √† quoi se r√©f√®re le mot ¬´‚ÄØit‚ÄØ¬ª ? M√™me si cela peut sembler facile, il peut √™tre difficile pour un algorithme de l‚Äôapprendre. C‚Äôest l√† qu‚Äôintervient l‚Äôauto-attention, qui peut apprendre une matrice d‚Äôattention pour le mot ¬´‚ÄØit‚ÄØ¬ª, o√π un poids important est attribu√© au mot ¬´‚ÄØanimal‚ÄØ¬ª.\n",
    "\n",
    "L‚Äôauto-attention permet √©galement au mod√®le d‚Äôapprendre √† interpr√©ter des mots ayant les m√™mes embeddings, comme apple, qui peut d√©signer une entreprise ou un fruit selon le contexte. Ce m√©canisme est tr√®s similaire √† l‚Äô√©tat cach√© que l‚Äôon trouve dans un r√©seau de neurones r√©current (RNN) (un autre type de r√©seau de neurones utilis√© pour traiter des donn√©es textuelles), mais ce processus, comme vous le verrez, permet au mod√®le de pr√™ter attention √† l‚Äôensemble de la s√©quence en parall√®le, ce qui autorise l‚Äôutilisation de s√©quences plus longues.\n",
    "\n",
    "L‚Äôauto-attention repose sur trois concepts :\n",
    "\n",
    "* Requ√™tes, cl√©s et valeurs\n",
    "* Attention par produit scalaire √† l‚Äô√©chelle\n",
    "* Masques\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "<figure>  \n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1VwPK-JVOe_NyY4QwKcaCxp4YGpxVIu1u\" alt=\"Vecteurs d‚Äôencodage positionnel\" width=\"800\"/>  \n",
    "  <figcaption><em></em></figcaption>  \n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnrWe3TsTpix"
   },
   "outputs": [],
   "source": [
    "class SequenceToQKV(nn.Module):\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, X):\n",
    "\n",
    "    # define the method for weight initialisation\n",
    "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
    "\n",
    "    # initialise three linear layers to do the QKV transformations.\n",
    "    # note: this can also be one layer, how do you think you would do it?\n",
    "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "\n",
    "    # transform and return the matrices\n",
    "    Q = q_layer(X)\n",
    "    K = k_layer(X)\n",
    "    V = v_layer(X)\n",
    "\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ovARyIGUt8M"
   },
   "source": [
    "Mais qu‚Äôest-ce que la requ√™te, la cl√© et la valeur ?\n",
    "\n",
    "üéØ **Analogie concr√®te :**\n",
    "Imaginez que vous √™tes dans une biblioth√®que, cherchant les informations les plus pertinentes pour r√©pondre √† une question.\n",
    "\n",
    "* Vous avez une question en t√™te : c‚Äôest votre **requ√™te** (query).\n",
    "* Chaque livre dans la biblioth√®que a un titre ou une description : c‚Äôest sa **cl√©** (key).\n",
    "* √Ä l‚Äôint√©rieur de chaque livre se trouve le contenu r√©el : c‚Äôest la **valeur** (value).\n",
    "\n",
    "Dans l‚Äôauto-attention, chaque mot d‚Äôune phrase joue les trois r√¥les :\n",
    "\n",
    "* Il cr√©e une requ√™te : ¬´ Que suis-je en train de chercher ? ¬ª\n",
    "* Il pr√©sente une cl√© : ¬´ Quelle information est contenue en moi ? ¬ª\n",
    "* Il offre une valeur : ¬´ Voici ce que je peux apporter. ¬ª\n",
    "\n",
    "En g√©n√©ral :\n",
    "\n",
    "* L‚Äôauto-attention est invariante √† la permutation (l‚Äôordre peut √™tre r√©arrang√© sans changer le r√©sultat).\n",
    "* L‚Äôauto-attention ne n√©cessite pas de param√®tres. Jusqu‚Äôici, l‚Äôinteraction entre les mots √©tait guid√©e par leurs embeddings et les encodages positionnels.\n",
    "* On s‚Äôattend √† ce que les valeurs le long de la diagonale (de la matrice) soient les plus √©lev√©es.\n",
    "* Si on ne souhaite pas que certaines positions interagissent, on peut toujours fixer leurs valeurs √† $-\\infty$.\n",
    "\n",
    "**Conclusion :** L‚Äôauto-attention permet au mod√®le de relier les mots entre eux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldE3HlzrMI0x"
   },
   "source": [
    "Maintenant que nous avons nos matrices `query`, `key` et `value`, il est temps de calculer la matrice d‚Äôattention. Rappelez-vous que dans tous les m√©canismes d‚Äôattention, il faut d‚Äôabord calculer un score pour chaque vecteur de la s√©quence, puis utiliser ces scores pour cr√©er un nouveau vecteur de contexte. Dans l‚Äôauto-attention, le calcul des scores se fait via le produit scalaire √† √©chelle (scaled dot product attention), puis les scores normalis√©s sont utilis√©s comme poids pour sommer les vecteurs valeurs et ainsi cr√©er le vecteur de contexte.\n",
    "\n",
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "o√π les scores d‚Äôattention sont calcul√©s par\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "puis ces scores sont multipli√©s par $V$ pour obtenir le vecteur de contexte.\n",
    "\n",
    "Ce qui se passe ici est similaire √† ce que nous avons vu avec l‚Äôattention par produit scalaire dans la section pr√©c√©dente, mais appliqu√© √† la s√©quence elle-m√™me. Pour chaque √©l√©ment de la s√©quence, on calcule la matrice des poids d‚Äôattention entre $q_i$ et $K$. Ensuite, on multiplie $V$ par chaque poids et on somme finalement tous les vecteurs pond√©r√©s $v_{\\text{weighted}}$ pour obtenir une nouvelle repr√©sentation pour $q_i$. Ainsi, on att√©nue les vecteurs moins pertinents et on renforce ceux importants dans la s√©quence lorsque notre attention est port√©e sur $q_1$.\n",
    "\n",
    "Le produit $QK^\\top$ est divis√© par la racine carr√©e de la dimension des vecteurs, $\\sqrt{d_k}$, afin d‚Äôassurer une meilleure stabilit√© des gradients lors de l‚Äôentra√Ænement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b5cyg7ALmKJ"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    Formula to return scaled dot product attention given QKV matrices\n",
    "    \"\"\"\n",
    "    d_k = key.shape[-1]\n",
    "\n",
    "    # get the raw scores (logits) from dot producting the queries and keys\n",
    "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
    "\n",
    "    # scale the raw scores and apply the softmax function to get the attention scores/weights\n",
    "    scaled_logits = logits / jnp.sqrt(d_k)\n",
    "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "\n",
    "    # multiply the weights by the value matrix to get the output\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFaCf5ipMNoF"
   },
   "source": [
    "Voyons maintenant l‚Äôattention par produit scalaire √† √©chelle en action. Nous prendrons une phrase, encoderons chaque mot avec word2vec, puis observerons √† quoi ressemblent les poids finaux de l‚Äôauto-attention.\n",
    "\n",
    "Nous n‚Äôutiliserons pas les couches de projection lin√©aire n√©cessaires pour entra√Æner ces matrices. Pour simplifier, nous allons poser $X = Q = V = K$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E_oxD2GLmM-"
   },
   "outputs": [],
   "source": [
    "# define a sentence\n",
    "sentence = \"I drink coke, but eat steak\"\n",
    "\n",
    "# embed and create QKV matrices\n",
    "word_embeddings, words = embed_sentence(sentence)\n",
    "Q = K = V = word_embeddings\n",
    "\n",
    "# calculate weights and plot\n",
    "outputs, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# plot the words and the attention weights between them\n",
    "words = remove_punctuation(sentence).split()\n",
    "print(attention_weights[0].shape, len(words))\n",
    "plot_attention_weight_matrix(attention_weights[0], words, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz_DkHbBMf4E"
   },
   "source": [
    "Gardez √† l‚Äôesprit que nous n‚Äôavons pas encore entra√Æn√© notre matrice d‚Äôattention. Cependant, en utilisant les vecteurs word2vec comme s√©quence, on peut d√©j√† observer que l‚Äôattention par produit scalaire √† √©chelle est capable de focaliser sur le mot ¬´‚ÄØeat‚ÄØ¬ª lorsque la requ√™te est ¬´‚ÄØsteak‚ÄØ¬ª, et que la requ√™te ¬´‚ÄØdrink‚ÄØ¬ª pr√™te plus d‚Äôattention √† ¬´‚ÄØcoke‚ÄØ¬ª et ¬´‚ÄØeat‚ÄØ¬ª.\n",
    "\n",
    "Plus de ressources :\n",
    "\n",
    "[Attention avec Q, K, V (vid√©o)](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ4lTjELMj68"
   },
   "source": [
    "### Attention masqu√©e\n",
    "\n",
    "L‚Äôattention masqu√©e est une technique utilis√©e dans les m√©canismes d‚Äôattention ‚Äî en particulier dans les transformeurs ‚Äî pour emp√™cher un mod√®le de ¬´ regarder vers l‚Äôavant ¬ª quand ce n‚Äôest pas autoris√©.\n",
    "\n",
    "üìö **Intuition : comme lire sans spoilers**\n",
    "Imaginez que vous lisez un roman policier chapitre par chapitre. Vous voulez deviner qui est le coupable sans sauter √† la fin. L‚Äôattention masqu√©e fonctionne de la m√™me fa√ßon :\n",
    "\n",
    "¬´ √Ä la position $t$, le mod√®le n‚Äôa le droit de pr√™ter attention qu‚Äôaux tokens √† la position $t$ ou avant, pas apr√®s. ¬ª\n",
    "\n",
    "üïµÔ∏è‚Äç‚ôÇÔ∏è **Pourquoi utiliser l‚Äôattention masqu√©e ?**\n",
    "\n",
    "1. üß± **Remplissage (padding) dans les s√©quences de longueurs in√©gales**\n",
    "   Lorsque l‚Äôon regroupe en batch des s√©quences (phrases ou s√©ries temporelles) de longueurs diff√©rentes, on compl√®te g√©n√©ralement les plus courtes avec des tokens de padding pour que toutes aient la m√™me taille. Mais ces tokens de padding ne contiennent aucune information r√©elle.\n",
    "\n",
    "‚ùó Si on ne les masque pas, le mod√®le pourrait les consid√©rer comme du contenu pertinent, ce qui perturberait l‚Äôapprentissage.\n",
    "\n",
    "2. üîí **Emp√™cher le regard vers l‚Äôavenir dans les mod√®les d√©codeurs**\n",
    "   Dans les mod√®les g√©n√©rateurs de s√©quences (comme GPT), on les entra√Æne en utilisant la phrase enti√®re en sortie. Mais lors de la g√©n√©ration r√©elle, le mod√®le ne doit voir que les tokens pass√©s et pr√©sents, pas ceux du futur.\n",
    "\n",
    "üß† Imaginez √©crire une histoire mot par mot. Vous ne devriez pas pouvoir lire la suite avant d‚Äôavoir √©crit le mot suivant !\n",
    "\n",
    "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"sch√©ma attention masqu√©e\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMdbuQLSMlqr"
   },
   "outputs": [],
   "source": [
    "# example of building a mask for tokens of size 32\n",
    "# the mask makes sure that positions only attend to previous positions in the input (causal mask)\n",
    "# we will use this later to insert -inf values into the raw scores\n",
    "mask = jnp.tril(jnp.ones((32, 32)))\n",
    "\n",
    "# plot\n",
    "sns.heatmap(mask, cmap=\"Blues\")\n",
    "plt.title(\"Example of mask that can be applied\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_7Wpv28Mp-M"
   },
   "source": [
    "Permet d√©sormais d'adapter notre fonction d'attention du produit DOT √† mise √† l'√©chelle pour impl√©menter l'attention masqu√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T9qSjMmMpOj"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot product attention with a causal mask (only allowed to attend to previous positions)\n",
    "    \"\"\"\n",
    "    d_k = key.shape[-1]\n",
    "    T_k = key.shape[-2]\n",
    "    T_q = query.shape[-2]\n",
    "\n",
    "    # get scaled logits using dot product as before\n",
    "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
    "    scaled_logits = logits / jnp.sqrt(d_k)\n",
    "\n",
    "    # add optional mask where values along the mask are set to -inf\n",
    "    if mask is not None:\n",
    "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
    "\n",
    "    # calcualte the attention weights via softmax\n",
    "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "\n",
    "    # sum with the values to get the output\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X31b1Pt6MvJ8"
   },
   "source": [
    "### La b√™te aux multiples t√™tes : l‚Äôattention multi-t√™te\n",
    "\n",
    "Nous avons parl√© du m√©canisme d‚Äôauto-attention dans la section pr√©c√©dente. Comment l‚Äôattention multi-t√™te se rapporte-t-elle √† ce m√©canisme d‚Äôauto-attention (attention par produit scalaire √† √©chelle) ?\n",
    "\n",
    "<figure>  \n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1e0C2tC29XylPRVfwXo_-NLzisQbLIdxl\" alt=\"Vecteurs d‚Äôencodage positionnel\" width=\"800\"/>  \n",
    "  <figcaption><em></em></figcaption>  \n",
    "</figure>  \n",
    "\n",
    "L‚Äôauto-attention multi-t√™te est une variante de l‚Äôauto-attention utilis√©e dans le mod√®le Transformer. Elle consiste √† ex√©cuter plusieurs calculs d‚Äôattention en parall√®le, chacun se focalisant sur diff√©rentes relations et aspects de la s√©quence d‚Äôentr√©e.\n",
    "\n",
    "Au lieu de calculer l‚Äôattention une seule fois, le m√©canisme MHA applique plusieurs fois en parall√®le l‚Äôattention par produit scalaire √† √©chelle. Selon l‚Äôarticle *Attention is All You Need*, ¬´ l‚Äôattention multi-t√™te permet au mod√®le de porter simultan√©ment attention √† l‚Äôinformation provenant de diff√©rents sous-espaces de repr√©sentation √† diff√©rentes positions. Avec une seule t√™te d‚Äôattention, la moyenne inhibe cela. ¬ª\n",
    "\n",
    "L‚Äôattention multi-t√™te peut √™tre vue comme une strat√©gie similaire √† l‚Äôempilement de noyaux de convolution dans une couche CNN (Convolution Neural Network). Cela permet aux noyaux de se concentrer et d‚Äôapprendre diff√©rentes caract√©ristiques et r√®gles, ce qui explique pourquoi plusieurs t√™tes d‚Äôattention fonctionnent aussi bien.\n",
    "\n",
    "<figure>  \n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1ulHkifKMzFSHl7-pJnUpc5VP-H2FssED\" alt=\"Sch√©ma attention multi-t√™te\" width=\"1300\" height=\"700\"/>  \n",
    "  <figcaption><em></em></figcaption>  \n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnOdT_r1M2U2"
   },
   "source": [
    "Ou plus pr√©cis√©ment quelque chose comme ceci: une pile d'attention du produit √† point √† l'√©chelle\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1lfMZAgs6bR5_0blSB95SAPuX1TNpNaCC\" alt=\"Positional Encoding Vectors\" width=\"500\"/>\n",
    "<Figcaption> <em> </em> </gigcaption>\n",
    "</ figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npyhGNKRM9CS"
   },
   "source": [
    "Voyons maintenant comment impl√©menter l‚Äôattention multi-t√™te. En termes simples, l‚Äôattention multi-t√™te revient √† ex√©cuter plusieurs fois en parall√®le le processus d‚Äôattention, en utilisant diff√©rentes copies des matrices $Q$, $K$ et $V$ pour chaque ¬´ t√™te ¬ª. Cela permet au mod√®le de se concentrer simultan√©ment sur diff√©rentes parties de l‚Äôentr√©e.\n",
    "\n",
    "Si vous souhaitez en savoir plus, consultez [cet article de blog de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) qui offre une explication d√©taill√©e.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN0q3iq9SMdn"
   },
   "source": [
    "### Attention par produit scalaire √† √©chelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kurPdKRQMuL_"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int  # Number of attention heads\n",
    "    d_m: int  # Dimension of the model's embeddings\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialize the sequence-to-QKV transformation module\n",
    "        self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
    "\n",
    "        # Define the initializer for the output linear layer weights\n",
    "        initializer = nn.initializers.variance_scaling(\n",
    "            scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\"\n",
    "        )\n",
    "\n",
    "        # Initialize the output projection layer Wo (used after attention)\n",
    "        self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
    "\n",
    "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
    "        # If Q, K, or V are not provided, use the input X to generate them\n",
    "        if None in [Q, K, V]:\n",
    "            assert not X is None, \"X has to be provided if either Q, K, or V are not provided\"\n",
    "\n",
    "            # Generate Q, K, and V matrices from the input X\n",
    "            Q, K, V = self.sequence_to_qkv(X)\n",
    "\n",
    "        # Extract the batch size (B), sequence length (T), and embedding size (d_m)\n",
    "        B, T, d_m = K.shape\n",
    "\n",
    "        # Calculate the size of each attention head's embedding (d_m / num_heads)\n",
    "        head_size = d_m // self.num_heads\n",
    "\n",
    "        # Reshape Q, K, V to have separate dimensions for the heads\n",
    "        # B, T, d_m -> B, T, num_heads, head_size -> B, num_heads, T, head_size\n",
    "        q_heads = Q.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
    "        k_heads = K.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
    "        v_heads = V.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
    "\n",
    "        # Apply scaled dot-product attention to each head\n",
    "        attention, attention_weights = scaled_dot_product_attention(\n",
    "            q_heads, k_heads, v_heads, mask\n",
    "        )\n",
    "\n",
    "        # Reshape the attention output back to its original dimensions\n",
    "        # (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, d_m)\n",
    "        attention = attention.swapaxes(1, 2).reshape(B, T, d_m)\n",
    "\n",
    "        # Apply the output linear transformation Wo to the attention output\n",
    "        X_new = self.Wo(attention)\n",
    "\n",
    "        # If return_weights is True, return both the transformed output and attention weights\n",
    "        if return_weights:\n",
    "            return X_new, attention_weights\n",
    "        else:\n",
    "            # Otherwise, return just the transformed output\n",
    "            return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF3tNzT_NGIm"
   },
   "source": [
    "\n",
    "## Que garder √† l'esprit:\n",
    "\n",
    "üîç Attention:\n",
    "\n",
    "* Capture des d√©pendances √† longue port√©e.\n",
    "\n",
    "* Permet la parall√©lisation (contrairement aux RNN).\n",
    "\n",
    "* L'attention est autochtone, sauf combinaison avec le codage positionnel.\n",
    "\n",
    "\n",
    "üí° Masque d'attention:\n",
    "\n",
    "* Masque causal (look-ahead): assure un comportement autor√©gressif (par exemple, le jeton T ne voit que des jetons ‚â§ t).\n",
    "\n",
    "* Masque de rembourrage: emp√™che l'attention aux positions rembourr√©es sans signification.\n",
    "\n",
    "* Impl√©ment√© en masquant les logits d'attention avant Softmax en utilisant $-\\\\infty$ (infinity).\n",
    "\n",
    "‚ú® Attention multiples\n",
    "\n",
    "* Ne vous contentez pas de regarder dans un sens - regardez plusieurs mod√®les √† la fois\n",
    "\n",
    "* Aide √† capturer plusieurs types de d√©pendances (par exemple, syntaxe, s√©mantique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5X4tRtSZxGHg"
   },
   "source": [
    "## üèóÔ∏è Entra√Ænement de votre propre LLM (Transformers) \\[<font color='green'>Interm√©diaire</font>]\n",
    "\n",
    "<!-- (30 minutes)-->\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "* Charger un jeu de donn√©es et entra√Æner un LLM\n",
    "* Visualiser les encodages positionnels \\[<font color='orange'>D√©butant</font>]\n",
    "* Impl√©menter :\n",
    "\n",
    "  * Encodages positionnels\n",
    "  * Bloc FFN\n",
    "  * Normalisation de couche (Layer norm)\n",
    "  * Bloc d√©codeur\n",
    "  * LLM complet \\[<font color='green'>Interm√©diaire</font>]\n",
    "* D√©finir la fonction de perte\n",
    "* Charger le jeu de donn√©es d‚Äôentra√Ænement\n",
    "* √âcrire le script d‚Äôentra√Ænement\n",
    "* Effectuer une inf√©rence avec le mod√®le entra√Æn√© \\[<font color='orange'>D√©butant</font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e71jR6TYRHEP"
   },
   "source": [
    "### Bloc de transformateur <Font Color = 'Green'> Interm√©diaire </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kobmi8IRHHA"
   },
   "source": [
    "Tout comme un Multi Layer perceptron MLP (un r√©seau de neurones simple qui traite les donn√©es d‚Äôentr√©e √† travers plusieurs couches) ou un Convolution Neural Network CNN (un type de r√©seau de neurones particuli√®rement efficace pour reconna√Ætre des motifs dans les images gr√¢ce √† des couches de convolution). Transformeurs sont compos√©s d‚Äôune pile de blocs de transformeurs. Dans cette section, nous allons construire chacun des composants n√©cessaires √† la cr√©ation d‚Äôun de ces blocs de transformeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yAG_MbgRWEs"
   },
   "source": [
    "\n",
    "#### Network Feed Award Network (FFN) / Multicouche Perceptron (MLP) <FONT COLOR = 'ORANGE'> d√©butant </font>\n",
    "\n",
    "\n",
    "<div style = \"Display: flex; align-items: Centre; justify-content: Centre; √©cart: 40px;\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1gyHqjfJUg_BLoFhAH6_KqsKxOQWvYtvD\" alt=\"Feed Forward Neural Network\" width=\"300\"/>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1H1pVFxJiSpM_Ozj1eKWNdcFQ5Hn5XsZz\" alt=\"Drawing\" width=\"260\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv5FUmp0RYd9"
   },
   "source": [
    "Dans le mod√®le original, ces blocs consistent en un simple MLP (Perceptron Multi-Couches) √† 2 couches utilisant la fonction d‚Äôactivation ReLU. Cependant, la fonction GeLU (Gaussian Error Linear Unit) est devenue tr√®s populaire, et nous l‚Äôutiliserons tout au long de ce pratique. La formule ci-dessous repr√©sente le r√©seau de neurones feedforward (FFN) avec activation GeLU. Dans ce r√©seau, l‚Äôentr√©e $x$ est d‚Äôabord pass√©e √† travers deux couches lin√©aires avec les poids $W_1$ et $W_2$, suivies des biais $b_1$ et $b_2$. La fonction d‚Äôactivation ReLU, souvent repr√©sent√©e par la fonction $\\max$, est ici remplac√©e par la fonction GeLU.\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2\n",
    "$$\n",
    "\n",
    "On peut interpr√©ter ce bloc comme traitant ce que le bloc d‚Äôattention multi-t√™te a produit, puis projetant ces nouvelles repr√©sentations de tokens dans un espace que le bloc suivant pourra exploiter plus efficacement. G√©n√©ralement, la premi√®re couche est tr√®s large, environ 2 √† 8 fois la taille des repr√©sentations de tokens. Cette architecture facilite la parall√©lisation des calculs pour une couche unique plus large pendant l‚Äôentra√Ænement, plut√¥t que de parall√©liser un bloc feedforward compos√© de plusieurs couches. Cela permet d‚Äôajouter plus de complexit√© tout en gardant un entra√Ænement et une inf√©rence optimis√©s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8mvJkVdRdZE"
   },
   "outputs": [],
   "source": [
    "# @title Code implementation for a feed forward neural network (Run me!)\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer MLP which widens then narrows the input.\n",
    "\n",
    "    Args:\n",
    "      widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
    "    \"\"\"\n",
    "    # widening_factor controls how much the input dimension is expanded in the first layer.\n",
    "    widening_factor: int = 4\n",
    "\n",
    "    # init_scale controls the scaling factor for weight initialization.\n",
    "    init_scale: float = 0.25\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Get the size of the last dimension of the input (embedding size).\n",
    "        d_m = x.shape[-1]\n",
    "\n",
    "        # Calculate the size of the first layer by multiplying the embedding size by the widening factor.\n",
    "        layer1_size = self.widening_factor * d_m\n",
    "\n",
    "        # Initialize the weights for both layers using a variance scaling initializer.\n",
    "        initializer = nn.initializers.variance_scaling(\n",
    "            scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
    "        )\n",
    "\n",
    "        # Define the first dense layer, which expands the input size.\n",
    "        layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
    "\n",
    "        # Define the second dense layer, which reduces the size back to the original dimension.\n",
    "        layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
    "\n",
    "        # Apply the first dense layer followed by a GELU activation function.\n",
    "        x = jax.nn.gelu(layer1(x))\n",
    "\n",
    "        # Apply the second dense layer to project the data back to its original dimension.\n",
    "        x = layer2(x)\n",
    "\n",
    "        # Return the final output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2Us0NGFRUPn"
   },
   "source": [
    "#### Bloc Add and Norm (Addition et Normalisation) <Font Color = 'Orange'> D√©butant </font>\n",
    "\n",
    "<div style = \"Display: flex; align-items: Centre; justify-content: Centre; √©cart: 40px;\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1lj8pqO6ttjbcTRUEW1rbtRlueiLPxoSr\" alt=\"Feed Forward Neural Network\" width=\"300\"/>\n",
    "  <img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-VmAFgbRkb3"
   },
   "source": [
    "Pour permettre aux transformeurs d‚Äôaller plus en profondeur, les connexions r√©siduelles sont cruciales car elles facilitent la circulation des gradients dans le r√©seau. Pour la normalisation, on utilise la **normalisation de couche** (layer norm). Celle-ci normalise chaque vecteur de token ind√©pendamment dans le batch. Il a √©t√© observ√© que normaliser ces vecteurs am√©liore la convergence et la stabilit√© des transformeurs.\n",
    "\n",
    "La normalisation de couche comporte deux param√®tres apprenables, `scale` et `bias`, qui rescalent la valeur normalis√©e. Pour chaque token d‚Äôentr√©e dans un batch, on calcule la moyenne $\\mu_i$ et la variance $\\sigma_i^2$. Puis on normalise le token par :\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "Ensuite, $\\hat{x}$ est rescal√© avec le `scale` appris $\\gamma$ et le `bias` $\\beta$ selon :\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta = \\mathrm{LN}_{\\gamma, \\beta}(x_i)\n",
    "$$\n",
    "\n",
    "Ainsi, notre bloc **Add & Norm** peut se repr√©senter par :\n",
    "\n",
    "$$\n",
    "\\mathrm{LN}(x + f(x))\n",
    "$$\n",
    "\n",
    "o√π $f(x)$ est soit un bloc MLP soit un bloc MHA (Multi-Head Attention).\n",
    "\n",
    "Pour impl√©menter ce bloc Add & Norm, on d√©finit un module Flax qui prend en entr√©e les donn√©es originales et les donn√©es trait√©es, les additionne, puis applique `flax.linen.nn.LayerNorm` sur la derni√®re dimension pour normaliser le r√©sultat. Cela permet de stabiliser l‚Äôentra√Ænement en standardisant la repr√©sentation somm√©e.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbbBqhEORmKz"
   },
   "outputs": [],
   "source": [
    "# @title Code implementation for Add and Norm block (Run me!)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"A block that implements the 'Add and Norm' operation\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, processed_x):\n",
    "        # Step 1: Add the original input (x) to the processed input (processed_x).\n",
    "        added = x + processed_x\n",
    "\n",
    "        # Step 2: Apply layer normalization to the result of the addition.\n",
    "        # - LayerNorm helps to stabilize and improve the training process by normalizing the output.\n",
    "        # - reduction_axes=-1 indicates that normalization is applied across the last dimension (typically the embedding dimension).\n",
    "        # - use_scale=True and use_bias=True allow the layer to learn scaling and bias parameters for further fine-tuning.\n",
    "        normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
    "\n",
    "        # Return the normalized result.\n",
    "        return normalised(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Z_7oRRRqPg"
   },
   "source": [
    "### Construire le d√©codeur de transformateur / llm <font color = 'vert'> interm√©diaire </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2xafT-jRtct"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
    "\n",
    "La majeure partie du travail pr√©paratoire est faite. Nous avons construit le bloc d‚Äôencodage positionnel, le bloc MHA, le bloc feed-forward et le bloc add\\&norm.\n",
    "\n",
    "La seule partie restante est de passer les entr√©es √† chaque bloc d√©codeur en appliquant le bloc MHA masqu√© (masked MHA) pr√©sent dans les blocs d√©codeurs.\n",
    "\n",
    "**T√¢che de code :** Impl√©mentez un module FLAX qui r√©alise la structure suivante pour le bloc d√©codeur :\n",
    "\n",
    "$$\n",
    "\\text{FFN} \\big( \\mathrm{Norm}(\\mathrm{MHA}(\\mathrm{Norm}(X))) \\big)\n",
    "$$\n",
    "\n",
    "Autrement dit, coder un module FLAX qui applique dans cet ordre :\n",
    "\n",
    "1. Normalisation sur l‚Äôentr√©e $X$\n",
    "2. Attention multi-t√™te masqu√©e (Masked MHA)\n",
    "3. Addition et normalisation\n",
    "4. R√©seau feed-forward (FFN)\n",
    "5. Addition et normalisation finale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JR03oeHRrZM"
   },
   "outputs": [],
   "source": [
    "#@title Decoder Block Implementation\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder block.\n",
    "\n",
    "    Args:\n",
    "        num_heads: The number of attention heads in the Multi-Head Attention (MHA) block.\n",
    "        d_m: The size of the token embeddings.\n",
    "        widening_factor: The factor by which the hidden layer size is expanded in the MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    num_heads: int\n",
    "    d_m: int\n",
    "    widening_factor: int = 4\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialize the Multi-Head Attention (MHA) block\n",
    "        self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
    "\n",
    "        # Initialize the AddNorm blocks for residual connections and normalization\n",
    "        self.add_norm1 = AddNorm()  # First AddNorm block after MHA\n",
    "        self.add_norm2 = AddNorm()  # Second AddNorm block after the MLP\n",
    "\n",
    "        # Initialize the FeedForwardBlock (MLP) which processes the data after attention\n",
    "        self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
    "\n",
    "    def __call__(self, X, mask=None, return_att_weight=True):\n",
    "        \"\"\"\n",
    "        Forward pass through the DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            X: Batch of input tokens fed into the decoder, shape [B, T_decoder, d_m]\n",
    "            mask [optional, default=None]: Mask to control which positions the attention is allowed to consider, shape [T_decoder, T_decoder].\n",
    "            return_att_weight [optional, default=True]: If True, returns the attention weights along with the output.\n",
    "\n",
    "        Returns:\n",
    "            If return_att_weight is True, returns a tuple (X, attention_weights_1).\n",
    "            Otherwise, returns the processed token representations X.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply Multi-Head Attention to the input tokens (X) with optional masking\n",
    "        attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
    "\n",
    "        # Apply the first AddNorm block (adds the original input X and normalizes)\n",
    "        X = self.add_norm1(X, attention)\n",
    "\n",
    "        # Pass the result through the FeedForwardBlock (MLP) to further process the data\n",
    "        projection = self.MLP(X)\n",
    "\n",
    "        # Apply the second AddNorm block (adds the input from the previous step and normalizes)\n",
    "        X = self.add_norm2(X, projection)\n",
    "\n",
    "        # Return the final output X, and optionally the attention weights\n",
    "        return (X, attention_weights_1) if return_att_weight else X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwyJ7wfvR3iB"
   },
   "source": [
    "Ensuite, nous allons tout assembler, en ajoutant les encodages de position ainsi que l'empilement de plusieurs blocs de transformateurs et l'ajout de notre couche de pr√©diction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOjE40uJR5sU"
   },
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model consisting of several layers of decoder blocks.\n",
    "\n",
    "    Args:\n",
    "        num_heads: Number of attention heads in each Multi-Head Attention (MHA) block.\n",
    "        num_layers: Number of decoder blocks in the model.\n",
    "        d_m: Dimensionality of the token embeddings.\n",
    "        vocab_size: Size of the vocabulary (number of unique tokens).\n",
    "        widening_factor: Factor by which the hidden layer size is expanded in the MLP.\n",
    "    \"\"\"\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    d_m: int\n",
    "    vocab_size: int\n",
    "    widening_factor: int = 4\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialize a list of decoder blocks, one for each layer in the model\n",
    "        self.blocks = [\n",
    "            DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "        # Initialize an embedding layer to convert token IDs into token embeddings\n",
    "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m)\n",
    "\n",
    "        # Initialize a dense layer for predicting the next token in the sequence\n",
    "        self.pred_layer = nn.Dense(self.vocab_size)\n",
    "\n",
    "    def __call__(self, X, mask=None, return_att_weights=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the LLM model.\n",
    "\n",
    "        Args:\n",
    "            X: Batch of input token IDs, shape [B, T_decoder] where B is batch size and T_decoder is sequence length.\n",
    "            mask [optional, default=None]: Mask to control which positions the attention can focus on, shape [T_decoder, T_decoder].\n",
    "            return_att_weights [optional, default=False]: Whether to return the attention weights.\n",
    "\n",
    "        Returns:\n",
    "            logits: The predicted probabilities for each token in the vocabulary.\n",
    "            If return_att_weights is True, also returns the attention weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert token IDs to embeddings (shape [B, T_decoder, d_m])\n",
    "        X = self.embedding(X)\n",
    "\n",
    "        # Get the sequence length of the input\n",
    "        sequence_len = X.shape[-2]\n",
    "\n",
    "        # Generate positional encodings and add them to the token embeddings\n",
    "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
    "        X = X + positions\n",
    "\n",
    "        # Initialize a list to store attention weights if needed\n",
    "        if return_att_weights:\n",
    "            att_weights = []\n",
    "\n",
    "        # Pass the embeddings through each decoder block in sequence\n",
    "        for block in self.blocks:\n",
    "            out = block(X, mask, return_att_weights)\n",
    "            if return_att_weights:\n",
    "                # If returning attention weights, unpack the output\n",
    "                X = out[0]\n",
    "                att_weights.append(out[1])\n",
    "            else:\n",
    "                # Otherwise, just update the input for the next block\n",
    "                X = out\n",
    "\n",
    "        # Apply a dense layer followed by a log softmax to get logits (predicted token probabilities)\n",
    "        logits = nn.log_softmax(self.pred_layer(X))\n",
    "\n",
    "        # Return the logits, and optionally, the attention weights\n",
    "        return logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAhAuP_wR9ml"
   },
   "source": [
    "Si tout est correct, si nous ex√©cutons le code ci-dessous, tout devrait s'ex√©cuter sans aucun probl√®me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvnKc43XR_c0"
   },
   "outputs": [],
   "source": [
    "# Define model dimensions and input shapes\n",
    "batch_size = 18\n",
    "sequence_length = 32\n",
    "embedding_dim = 16\n",
    "num_decoder_layers = 8\n",
    "vocab_size = 25670\n",
    "\n",
    "# Initialize the language model\n",
    "llm = LLM(\n",
    "    num_heads=1,\n",
    "    num_layers=1,\n",
    "    d_m=embedding_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    widening_factor=4\n",
    ")\n",
    "\n",
    "# Create a lower-triangular attention mask for causal decoding\n",
    "causal_mask = jnp.tril(np.ones((sequence_length, sequence_length)))\n",
    "\n",
    "# Generate random input token IDs\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "input_token_ids = jax.random.randint(rng_key, [batch_size, sequence_length], 0, vocab_size)\n",
    "\n",
    "# Initialize model parameters\n",
    "model_params = llm.init(rng_key, input_token_ids, mask=causal_mask)\n",
    "\n",
    "# Run the model and extract logits and attention weights from the decoder\n",
    "logits, decoder_attention_weights = llm.apply(\n",
    "    model_params,\n",
    "    input_token_ids,\n",
    "    mask=causal_mask,\n",
    "    return_att_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8vowzz-SC2q"
   },
   "source": [
    "En tant que v√©rification finale de la sant√© mentale, nous pouvons confirmer que nos poids d'attention fonctionnent correctement.Comme le montre la figure ci-dessous, les poids d'attention du d√©codeur ne se concentrent que sur les jetons pr√©c√©dents, comme pr√©vu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NbvRSNzSF0H"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plt.suptitle(\"LLM attention weights\")\n",
    "sns.heatmap(decoder_attention_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nsFaXhdSKZG"
   },
   "source": [
    "### Entra√Ænement de votre LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6BUm34sSRJH"
   },
   "source": [
    "#### Objectif de formation [<Font Color = 'Green'> Interm√©diaire </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jJ2TTwjSV-u"
   },
   "source": [
    "Une phrase n‚Äôest rien d‚Äôautre qu‚Äôune suite de mots. Un LLM vise √† pr√©dire le mot suivant en tenant compte du contexte actuel, c‚Äôest-√†-dire des mots qui l‚Äôont pr√©c√©d√©.\n",
    "\n",
    "Voici l‚Äôid√©e principale :\n",
    "\n",
    "Pour calculer la probabilit√© qu‚Äôune phrase compl√®te ¬´ mot1, mot2, ..., dernier mot ¬ª apparaisse dans un contexte donn√© $c$, on d√©compose la phrase en mots individuels et on consid√®re la probabilit√© de chaque mot sachant les mots qui le pr√©c√®dent. Ces probabilit√©s individuelles sont ensuite multipli√©es entre elles :\n",
    "\n",
    "$$\n",
    "\\text{Probabilit√© de la phrase} = \\text{Probabilit√© de mot}_1 \\times \\text{Probabilit√© de mot}_2 \\times \\ldots \\times \\text{Probabilit√© du dernier mot}\n",
    "$$\n",
    "\n",
    "Cette m√©thode revient √† construire un r√©cit morceau par morceau en se basant sur ce qui a √©t√© racont√© avant.\n",
    "\n",
    "Math√©matiquement, cela s‚Äôexprime par la vraisemblance (probabilit√©) d‚Äôune s√©quence de mots $y_1, y_2, \\ldots, y_n$ dans un contexte donn√© $c$, calcul√©e en multipliant les probabilit√©s de chaque mot $y_t$ conditionn√©es aux mots pr√©c√©dents $(y_{<t})$ et au contexte $c$ :\n",
    "\n",
    "$$\n",
    "P(y_1, y_2, \\ldots, y_n \\mid c) = \\prod_{t=1}^{n} P(y_t \\mid y_{<t}, c)\n",
    "$$\n",
    "\n",
    "Ici, $y_{<t}$ d√©signe la s√©quence $y_1, y_2, \\ldots, y_{t-1}$, tandis que $c$ repr√©sente le contexte.\n",
    "\n",
    "C‚Äôest analogue √† assembler un puzzle o√π la pi√®ce suivante est plac√©e en fonction de celles d√©j√† pos√©es.\n",
    "\n",
    "---\n",
    "\n",
    "Gardez √† l‚Äôesprit que lors de l‚Äôentra√Ænement d‚Äôun transformeur, on ne travaille pas avec des mots mais avec des tokens. Pendant le processus d‚Äôentra√Ænement, les param√®tres du mod√®le sont ajust√©s en calculant la perte d‚Äôentropie crois√©e entre le token pr√©dit et le token correct, puis en effectuant la r√©tropropagation. La perte au temps $t$ est calcul√©e ainsi :\n",
    "\n",
    "$$\n",
    "\\text{Loss}_t = - \\sum_{w \\in V} y_t \\log(\\hat{y}_t)\n",
    "$$\n",
    "\n",
    "o√π $w \\in V$ correspond √† chaque mot $w$ dans le vocabulaire $V$.\n",
    "\n",
    "Ici, $y_t$ est le token r√©el au temps $t$, et $\\hat{y}_t$ est le token pr√©dit par le mod√®le au m√™me instant. La perte sur toute la phrase est ensuite calcul√©e comme :\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{phrase}} = \\frac{1}{n} \\sum_{t=1}^n \\text{Loss}_t\n",
    "$$\n",
    "\n",
    "avec $n$ la longueur de la s√©quence.\n",
    "\n",
    "Ce processus it√©ratif affine progressivement les capacit√©s pr√©dictives du mod√®le.\n",
    "\n",
    "---\n",
    "\n",
    "**T√¢che de code :** Impl√©mentez la fonction de perte d‚Äôentropie crois√©e (cross-entropy loss) ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeAWLPJbSVLM"
   },
   "outputs": [],
   "source": [
    "def sequence_loss_fn(logits, targets):\n",
    "  '''\n",
    "  Compute the cross-entropy loss between predicted token ID and true ID.\n",
    "\n",
    "  Args:\n",
    "    logits: An array of shape [batch_size, sequence_length, vocab_size]\n",
    "    targets: The targets we are trying to predict\n",
    "\n",
    "  Returns:\n",
    "    loss: A scalar value representing the mean batch loss\n",
    "  '''\n",
    "\n",
    "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
    "  assert logits.shape == target_labels.shape\n",
    "\n",
    "  mask = jnp.greater(targets, 0)\n",
    "\n",
    "  # Hint: Compute the cross-entropy loss by first applying `jax.nn.log_softmax(logits)`\n",
    "  # to get the log probabilities for each class. Then, multiply these log probabilities\n",
    "  # by the `target_labels` to focus on the correct class's probability. Sum this result\n",
    "  # along the last axis to get the loss for each token. Finally, apply the mask to the loss,\n",
    "  # sum the masked losses, and normalize by the number of non-padding tokens.\n",
    "  loss = ...# FINISH ME\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwp5yuhrSdgB"
   },
   "outputs": [],
   "source": [
    "# @title Run me to test your code\n",
    "VOCAB_SIZE = 25670\n",
    "targets = jnp.array([[0, 2, 0]])\n",
    "key = jax.random.PRNGKey(42)\n",
    "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
    "loss = sequence_loss_fn(X, targets)\n",
    "real_loss = jnp.array(10.966118)\n",
    "\n",
    "try:\n",
    "  jnp.allclose(real_loss, loss)\n",
    "  print(\"It seems correct. Look at the answer below to compare methods.\")\n",
    "except:\n",
    "  print(\"Not returning the correct value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIPqzOq1Sg9p"
   },
   "outputs": [],
   "source": [
    "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
    "def sequence_loss_fn(logits, targets):\n",
    "    \"\"\"Compute the sequence loss between predicted logits and target labels.\"\"\"\n",
    "\n",
    "    # Convert the target indices to one-hot encoded vectors.\n",
    "    # Each target label is converted into a one-hot vector of size VOCAB_SIZE.\n",
    "    target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
    "\n",
    "    # Ensure that the shape of logits matches the shape of the one-hot encoded targets.\n",
    "    # This is important because we need to compute the loss across matching dimensions.\n",
    "    assert logits.shape == target_labels.shape\n",
    "\n",
    "    # Create a mask that ignores padding tokens in the loss calculation.\n",
    "    # The mask is True (1) where the target value is greater than 0 and False (0) otherwise.\n",
    "    mask = jnp.greater(targets, 0)\n",
    "\n",
    "    # Compute the cross-entropy loss for each token.\n",
    "    # Cross-entropy is calculated as the negative log probability of the correct class.\n",
    "    # jax.nn.log_softmax(logits) gives us the log probabilities for each class.\n",
    "    # We multiply by the target_labels to select the log probability of the correct class.\n",
    "    loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
    "\n",
    "    # Apply the mask to the loss to ignore padding positions and sum up the losses.\n",
    "    # We then normalize the total loss by the number of non-padding tokens.\n",
    "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jp_1cbQSnzq"
   },
   "source": [
    "#### Mod√®les d'entra√Ænement [<font color = 'vert'> interm√©diaire </font>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWx1nrZZSmZj"
   },
   "source": [
    "Dans la section suivante, nous d√©finissons tous les processus n√©cessaires pour former le mod√®le en utilisant l'objectif d√©crit ci-dessus.Une grande partie de cela est maintenant le travail requis pour faire une formation en utilisant le lin.\n",
    "\n",
    "Ci-dessous, nous rassemblons l'ensemble de donn√©es et nous allons nous entra√Æner, qui est l'ensemble de donn√©es Shakespeare de Karpathy.Il n'est pas si important de comprendre ce code, donc soit ex√©cuter la cellule pour charger les donn√©es, soit afficher le code si vous voulez le comprendre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MyUYK9DSqbo"
   },
   "outputs": [],
   "source": [
    "# @title Create Shakespeare dataset and iterator (optional, but run the cell)\n",
    "\n",
    "# Trick to avoid errors when downloading tinyshakespeare.\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
    "\n",
    "class WordBasedAsciiDatasetForLLM:\n",
    "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
    "\n",
    "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
    "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "        # Tokenize by splitting the text into words\n",
    "        words = corpus.split()\n",
    "        self.vocab_size = len(set(words))  # Number of unique words\n",
    "\n",
    "        # Create a mapping from words to unique IDs\n",
    "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
    "\n",
    "        # Store the inverse mapping from IDs to words\n",
    "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
    "\n",
    "        # Convert the words in the corpus to their corresponding IDs\n",
    "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
    "\n",
    "        crop_len = sequence_length + 1\n",
    "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
    "        if ragged:\n",
    "            corpus = corpus[:-ragged]\n",
    "        corpus = corpus.reshape([-1, crop_len])\n",
    "\n",
    "        if num_batches < 10:\n",
    "            raise ValueError(\n",
    "                f\"Only {num_batches} batches; consider a shorter \"\n",
    "                \"sequence or a smaller batch.\"\n",
    "            )\n",
    "\n",
    "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
    "            corpus, batch_size * 10\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield next mini-batch.\"\"\"\n",
    "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
    "        batch = np.stack(batch)\n",
    "        # Create the language modeling observation/target pairs.\n",
    "        return dict(\n",
    "            input=batch[:, :-1], target=batch[:, 1:]\n",
    "        )\n",
    "\n",
    "    def ids_to_words(self, ids):\n",
    "        \"\"\"Convert a sequence of word IDs to words.\"\"\"\n",
    "        return [self.id_to_word[id] for id in ids]\n",
    "\n",
    "    @staticmethod\n",
    "    def _infinite_shuffle(iterable, buffer_size):\n",
    "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
    "        ds = itertools.cycle(iterable)\n",
    "        buf = [next(ds) for _ in range(buffer_size)]\n",
    "        random.shuffle(buf)\n",
    "        while True:\n",
    "            item = next(ds)\n",
    "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
    "            result, buf[idx] = buf[idx], item\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJf97wcmSv2y"
   },
   "source": [
    "Permet maintenant de voir comment nos donn√©es sont structur√©es pour la formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3UEb_1hSzPc"
   },
   "outputs": [],
   "source": [
    "# sample and look at the data\n",
    "batch_size = 2\n",
    "seq_length = 32\n",
    "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
    "\n",
    "batch = next(train_dataset)\n",
    "\n",
    "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
    "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
    "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
    "    print(\"ASCII:\", obs)\n",
    "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
    "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
    "    print(\"ASCII:\", target)\n",
    "\n",
    "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
    "\n",
    "VOCAB_SIZE = train_dataset.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVKnUqggS5ru"
   },
   "source": [
    "Ensuite, entra√Ænons notre LLM et voyons comment elle fonctionne dans la production de texte shakespearien.Tout d'abord, nous d√©finirons ce qui se passe pour chaque √©tape de formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMNCtFxxS91p"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
    "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
    "    \"\"\"\n",
    "    Perform a single training step.\n",
    "\n",
    "    Args:\n",
    "        params: The current parameters of the model.\n",
    "        optimizer_state: The current state of the optimizer.\n",
    "        batch: A dictionary containing the input data and target labels for the batch.\n",
    "        apply_fn: The function used to apply the model to the inputs.\n",
    "        update_fn: The function used to update the model parameters based on the gradients.\n",
    "\n",
    "    Returns:\n",
    "        Updated parameters, updated optimizer state, and the computed loss for the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        # Get the sequence length (T) from the input data.\n",
    "        T = batch['input'].shape[1]\n",
    "\n",
    "        # Apply the model to the input data, using a lower triangular mask to enforce causality.\n",
    "        # jnp.tril(np.ones((T, T))) creates a lower triangular matrix of ones.\n",
    "        logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
    "\n",
    "        # Calculate the loss between the predicted logits and the target labels.\n",
    "        loss = sequence_loss_fn(logits, batch['target'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Compute the loss and its gradients with respect to the parameters.\n",
    "    loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    # Update the optimizer state and calculate the parameter updates based on the gradients.\n",
    "    updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
    "\n",
    "    # Apply the updates to the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    # Return the updated parameters, optimizer state, and the loss for the batch.\n",
    "    return params, optimizer_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12uUhzgcTRBM"
   },
   "source": [
    "Ensuite, nous initialisons notre optimiseur et notre mod√®le.N'h√©sitez pas √† jouer avec les hyperparam√®tres pendant la pratique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NJjGy-qTSk8"
   },
   "outputs": [],
   "source": [
    "# Define all hyperparameters\n",
    "d_model = 128            # Dimension of token embeddings (d_m)\n",
    "num_heads = 4            # Number of attention heads in Multi-Head Attention\n",
    "num_layers = 1           # Number of decoder blocks in the model\n",
    "widening_factor = 2      # Factor to widen the hidden layer size in the MLP\n",
    "LR = 2e-3                # Learning rate for the optimizer\n",
    "batch_size = 32          # Number of samples per training batch\n",
    "seq_length = 64          # Length of each input sequence (number of tokens)\n",
    "\n",
    "# Set up the training data\n",
    "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
    "vocab_size = train_dataset.vocab_size  # Get the size of the vocabulary from the dataset\n",
    "batch = next(train_dataset)            # Get the first batch of input data\n",
    "\n",
    "# Set the random number generator key for model initialization\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize the LLM model with the specified hyperparameters\n",
    "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
    "\n",
    "# Create a causal mask to ensure that the model only attends to previous tokens\n",
    "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
    "\n",
    "# Initialize the model parameters using the first batch of input data and the mask\n",
    "params = llm.init(rng, batch['input'], mask)\n",
    "\n",
    "# Set up the optimizer using the Adam optimization algorithm with the specified learning rate\n",
    "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
    "optimizer_state = optimizer.init(params)  # Initialize the optimizer state with the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdwUHcn6TZQT"
   },
   "source": [
    "Maintenant, nous nous entra√Ænons!Cela prendra quelques minutes .. Pendant qu'il s'entra√Æne, avez-vous encore salu√© votre voisin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vlk7laQVTYNw"
   },
   "outputs": [],
   "source": [
    "plotlosses = PlotLosses()\n",
    "\n",
    "MAX_STEPS = 3500\n",
    "LOG_EVERY = 32\n",
    "losses = []\n",
    "VOCAB_SIZE = 25670\n",
    "\n",
    "# Training loop\n",
    "for step in range(MAX_STEPS):\n",
    "    batch = next(train_dataset)\n",
    "    params, optimizer_state, loss = train_step(\n",
    "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
    "    losses.append(loss)\n",
    "    if step % LOG_EVERY == 0:\n",
    "        loss_ = jnp.array(losses).mean()\n",
    "        plotlosses.update(\n",
    "            {\n",
    "                \"loss\": loss_,\n",
    "            }\n",
    "        )\n",
    "        plotlosses.send()\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE5N87UWT_uK"
   },
   "source": [
    "#### Inspectant le LLM Entrain√© [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGBOSUgdUE3J"
   },
   "source": [
    "**Rappel :** n‚Äôoubliez pas d‚Äôex√©cuter tout le code pr√©sent√© jusqu‚Äô√† pr√©sent dans cette section avant de lancer les cellules ci-dessous !\n",
    "\n",
    "G√©n√©rons maintenant du texte et voyons les performances de notre mod√®le. NE PAS ARR√äTER LA CELLULE UNE FOIS QU‚ÄôELLE EST EN COURS D‚ÄôEX√âCUTION, CELA POURRAIT FAIRE PLANTER LA SESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4XVDaTHUICU"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(2, ))\n",
    "def generate_prediction(params, input, apply_fn):\n",
    "  logits = apply_fn(params, input)\n",
    "  argmax_out = jnp.argmax(logits, axis=-1)\n",
    "  return argmax_out[0][-1].astype(int)\n",
    "\n",
    "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
    "    '''\n",
    "    Get the model output\n",
    "    '''\n",
    "\n",
    "    prompt = \"Love\"\n",
    "    print(prompt, end=\"\")\n",
    "    tokens = prompt.split()\n",
    "\n",
    "    # predict and append\n",
    "    for i in range(15):\n",
    "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
    "      prediction = generate_prediction(params, input, llm.apply)\n",
    "      prediction = id_2_word[int(prediction)]\n",
    "      tokens.append(prediction)\n",
    "      print(\" \"+prediction, end=\"\")\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "id_2_word = train_dataset.id_to_word\n",
    "word_2_id = train_dataset.word_to_id\n",
    "\n",
    "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_7rSlfaUOeD"
   },
   "source": [
    "Enfin, nous avons impl√©ment√© tout ce qui pr√©c√®de en s√©lectionnant l‚ÄôID du token avec la probabilit√© maximale d‚Äô√™tre correct. C‚Äôest ce qu‚Äôon appelle le d√©codage glouton (greedy decoding), car on ne prend que le token le plus probable. Cela a bien fonctionn√© dans ce cas, mais dans certaines situations, cette approche peut entra√Æner une d√©gradation des performances, notamment lorsqu‚Äôon cherche √† g√©n√©rer un texte r√©aliste.\n",
    "\n",
    "D‚Äôautres m√©thodes existent pour l‚Äô√©chantillonnage depuis le d√©codeur, parmi lesquelles l‚Äôalgorithme bien connu appel√© recherche par faisceau (beam search). Nous fournissons ci-dessous des ressources pour ceux qui souhaitent en apprendre davantage.\n",
    "\n",
    "[D√©codage glouton (Greedy Decoding)](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
    "\n",
    "[Recherche par faisceau (Beam Search)](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tixtBEtRPZ5n"
   },
   "source": [
    "## Mati√®re √† r√©flexion : Quel est le co√ªt lorsque vous discutez avec un LLM dans votre langue ?\n",
    "\n",
    "Avant de clore ce chapitre sur les Transformers, nous souhaitons vous inviter √† r√©fl√©chir √† ceci : quel est le co√ªt lorsque vous √©changez avec un LLM dans votre langue ?\n",
    "\n",
    "Le co√ªt d‚Äôinteraction avec un mod√®le de langage d√©pend du nombre de tokens dans votre message. En effet, les LLM facturent √† la token, pas au mot, √† la phrase ou au caract√®re.\n",
    "\n",
    "Par exemple :\n",
    "\n",
    "* **GPT-4.0-turbo** (en date de juin 2025) [co√ªte](https://openai.com/api/pricing/) **2 \\$ pour 1 million de tokens** en entr√©e.\n",
    "* **Gemma 2.5 Pro** (via l‚ÄôAPI Gemini, juin 2025) [co√ªte](https://ai.google.dev/gemini-api/docs/pricing) **1,25 \\$ pour 1 million de tokens**.\n",
    "\n",
    "Comme chaque mod√®le utilise une m√©thode de tokenisation diff√©rente, ils d√©composent les langues diff√©remment. Cela peut entra√Æner des co√ªts plus √©lev√©s pour certaines langues, m√™me lorsque la phrase signifie exactement la m√™me chose.\n",
    "\n",
    "### Calculons le co√ªt des tokens\n",
    "\n",
    "Si le co√ªt est de **2 \\$ pour 1 million de tokens**, voici comment le co√ªt √©volue :\n",
    "\n",
    "$$\n",
    "\\text{Co√ªt} = \\text{Nombre de tokens} \\times \\left(\\frac{2}{1{,}000{,}000}\\right)\n",
    "$$\n",
    "\n",
    "#### üí∞ Estimations exemples :\n",
    "\n",
    "| Nombre de tokens | Calcul                                      | Co√ªt (USD)     |\n",
    "| ---------------- | ------------------------------------------- | -------------- |\n",
    "| 10 tokens        | \\$10 \\times \\frac{2}{1{,}000{,}000}\\$       | **0,00002 \\$** |\n",
    "| 100 tokens       | \\$100 \\times \\frac{2}{1{,}000{,}000}\\$      | **0,0002 \\$**  |\n",
    "| 1 000 tokens     | \\$1000 \\times \\frac{2}{1{,}000{,}000}\\$     | **0,002 \\$**   |\n",
    "| 10 000 tokens    | \\$10{,}000 \\times \\frac{2}{1{,}000{,}000}\\$ | **0,02 \\$**    |\n",
    "\n",
    "Imaginez maintenant que vous g√©n√©rez ou traitez des millions de requ√™tes dans une langue locale qui se tokenise de mani√®re inefficace. Cela pourrait signifier d√©penser plus pour dire la m√™me chose, simplement parce que votre langue ne s‚Äôadapte pas bien au tokenizer utilis√© par le mod√®le de langage.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1m0mCSEEuBxNzb8pJfMANqsvikRorBubE\" alt=\"ChatGPT Pricing\" width=\"800\"/>\n",
    "  <figcaption><em></em></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVTduxk4PdYC"
   },
   "source": [
    "#### üí∏ Combien co√ªte ma LLM ? ‚Äî Tokenisation en code\n",
    "\n",
    "**Configuration :**\n",
    "Nous allons tester comment diff√©rents mod√®les tokenisent la m√™me phrase dans plusieurs langues, en utilisant :\n",
    "\n",
    "* **GPT-2** (tokenizer g√©n√©ral pour l‚Äôanglais)\n",
    "* **Gemma** (tokenizer multilingue)\n",
    "* Un **tokenizer sp√©cifique √† une langue**\n",
    "\n",
    "Commen√ßons par **GPT-2**.\n",
    "\n",
    "> Pour tous les mod√®les compar√©s ci-dessous, nous utiliserons le co√ªt du token GPT-4.1 comme base de comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlLkCsH4Amkt"
   },
   "outputs": [],
   "source": [
    "def token_cost(tokens: list, model_name: str):\n",
    "    \"\"\"\n",
    "    Function that takes in a list of tokens and returns the token cost for the given model.\n",
    "    \"\"\"\n",
    "    # cost per token for Gemma 2.5 Pro https://ai.google.dev/gemini-api/docs/pricing\n",
    "    # for now, assume all tokenizer cost the same\n",
    "    cost_per_token = 2/1000000  # cost per token for GPT4.1\n",
    "\n",
    "    return len(tokens) * cost_per_token  # Gemma3 uses a fixed cost per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wO5Y7_NPmqP"
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  #@param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
    "tokenizer = get_tokenizer(model_name)  # Default tokenizer, can be changed as needed\n",
    "sentence = \"This is a sample sentence for tokenization.\" #@param {type:\"string\"}\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens:\",  tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"Token cost:\", token_cost(tokens, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-Svrwz-PsEo"
   },
   "source": [
    "Next,\n",
    "- Essayez une m√™me phrase dans une langue diff√©rente, par exemple Swahili ou Yoruba\n",
    "- Observer et enregistrer le nombre de jetons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5qcyoRWPrBJ"
   },
   "outputs": [],
   "source": [
    "sentences = {\"English\": \"Welcome to the Indaba LLM tutorial happening in Kigali. Get ready to explore the world of LLMs.\",\n",
    "             \"German\": \"Willkommen zum Indaba LLM Tutorial in Kigali. Macht euch bereit die Welt von LLMs zu erkunden.\",\n",
    "             \"French\": \"Bienvenue au tutoriel Indaba sur les LLM qui se d√©roule √† Kigali. Pr√©parez-vous √† explorer le monde des LLM.\",\n",
    "             \"Lithuania\": \"Sveiki atvykƒô ƒØ Indaba LLM (Did≈æi≈≥j≈≥ Kalbini≈≥ Modeli≈≥) mokymus vykstanƒçius Kigalyje. Pasiruo≈°kite tyrinƒóti LLM pasaulƒØ.\",\n",
    "             \"Yoruba\": \"Kaab·ªç si ik·∫πk·ªç Indaba LLM ti n ·π£·∫πl·∫π ni Kigali. ·π¢etan lati ·π£awari agbaye ti LLMs.\",\n",
    "             \"Swahili\": \"Karibu kwenye mafunzo ya Indaba LLM yanayofanyika Kigali. Jitayarishe kuchunguza ulimwengu wa LLM.\",\n",
    "             \"Arabic\":  \".ŸÖÿ±ÿ≠ÿ®Ÿãÿß ÿ®ŸÉŸÖ ŸÅŸä Ÿàÿ±ÿ¥ÿ© ÿπŸÖŸÑ  ÿ≠ŸàŸÑ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ŸàŸäÿ© ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ÿßŸÑÿ™Ÿä ÿ™ŸèŸÇÿßŸÖ ŸÅŸä ŸÉŸäÿ∫ÿßŸÑŸä. ÿßÿ≥ÿ™ÿπÿØŸàÿß ŸÑÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ ÿπÿßŸÑŸÖ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ŸàŸäÿ© ÿßŸÑŸÉÿ®Ÿäÿ±ÿ©\",  # The arabic sentence is not a 1-1 translation\n",
    "             \"Kinyarwanda\": \"Murakaza neza mu isomo rya Indaba LLM riri kubera i Kigali. Mwitegure kuvumbura isi ya za LLM.\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhPM1MyDPw8N"
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"   #@param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
    "for language, sentence in sentences.items():\n",
    "    # For each language, tokenize the sentence and print the results\n",
    "    tokens, token_ids = tokenize(sentence, model_name)\n",
    "    print(f\"Language: {language}, Model: {model_name}\")\n",
    "    print(\"-\" * 50)  # Separator for clarity\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    print(\"Token count:\", len(tokens))\n",
    "    print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "    print(\"-\" * 50)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD2dDhQ2P2cU"
   },
   "source": [
    "Avez-vous remarqu√© comment GPT-2 segmente le texte arabe caract√®re par caract√®re, souvent octet par octet, au lieu de saisir des unit√©s significatives ? C‚Äôest parce que GPT-2 a √©t√© principalement entra√Æn√© sur des donn√©es en anglais et n‚Äôa pas √©t√© optimis√© pour g√©rer l‚Äôarabe. Bien qu‚Äôil soit possible de tokenizer l‚Äôarabe avec le tokenizer de GPT-2, cela conduit g√©n√©ralement √† un nombre de tokens bien plus √©lev√© compar√© au m√™me contenu en anglais ‚Äî ce qui implique √©galement un co√ªt plus √©lev√©.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alop4gEjP48e"
   },
   "source": [
    "Essayons le tokenizer [Gemma 3](https://developers.googleblog.com/en/introducing-gemma3/#:~:text=Gemma%203%20uses%20a%20new,TPUs%20using%20the%20JAX%20Framework.). Il s‚Äôagit d‚Äôun nouveau tokenizer multilingue con√ßu pour prendre en charge plus de 140 langues.\n",
    "\n",
    "> Nous utiliserons le co√ªt du token GPT-4.1 comme base de comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajJMBSYkP09h"
   },
   "outputs": [],
   "source": [
    "model_name = \"gemma3\"\n",
    "for language, sentence in sentences.items():\n",
    "    # For each language, tokenize the sentence and print the results\n",
    "    tokens, token_ids = tokenize(sentence, model_name)\n",
    "    print(f\"Language: {language}, Model: {model_name}\")\n",
    "    print(\"-\" * 50)  # Separator for clarity\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    print(\"Token count:\", len(tokens))\n",
    "    print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "    print(\"-\" * 50)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8EekZSfP_Lz"
   },
   "source": [
    "Qu‚Äôen est-il de l‚Äôutilisation d‚Äôun tokenizer sp√©cifique √† une langue ? Par exemple, essayez `asafaya/bert-base-arabic` sur un texte en arabe ‚Äî il est con√ßu pour g√©rer bien mieux la structure et les nuances de la langue que les tokenizers g√©n√©ralistes. Remarquez comment le nombre de tokens ‚Äî et donc le co√ªt ‚Äî diminue consid√©rablement lorsque vous utilisez un tokenizer sp√©cifiquement adapt√© √† l‚Äôarabe ?\n",
    "\n",
    "> Nous utiliserons le co√ªt du token GPT-4.1 comme base de comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjzJYNccP9uZ"
   },
   "outputs": [],
   "source": [
    "model_name = \"asafaya/bert-base-arabic\"\n",
    "language = \"Arabic\"\n",
    "sentence = sentences[language]  # Get the Arabic sentence from the dictionary\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "clear_output()\n",
    "print(f\"Language: {language}, Model: {model_name}\")\n",
    "print(\"-\" * 50)  # Separator for clarity\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "print(\"-\" * 50)  # Separator for clarity\n",
    "\n",
    "\n",
    "model_name = \"gemma3\"\n",
    "language = \"Arabic\"\n",
    "sentence = sentences[language]  # Get the Arabic sentence from the dictionary\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "print(f\"Language: {language}, Model: {model_name}\")\n",
    "print(\"-\" * 50)  # Separator for clarity\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "print(\"-\" * 50)  # Separator for clarity\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "language = \"Arabic\"\n",
    "sentence = sentences[language]  # Get the Arabic sentence from the dictionary\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "print(f\"Language: {language}, Model: {model_name}\")\n",
    "print(\"-\" * 50)  # Separator for clarity\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "print(\"-\" * 50)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qH-jn4JvQGJW"
   },
   "outputs": [],
   "source": [
    "# Define the models you want to compare\n",
    "models = [\"asafaya/bert-base-arabic\", \"gemma3\", \"gpt2\"]\n",
    "costs = []\n",
    "language = \"Arabic\"  # Language to use for the cost comparison\n",
    "# Calculate the token cost for each model using the sentences dictionary\n",
    "for model in models:\n",
    "    total_cost = 0\n",
    "    tokens, _ = tokenize(sentences[language], model)\n",
    "    total_cost = token_cost(tokens, model)\n",
    "    costs.append(total_cost * 1000000)  # Scale by 1 million for display\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(models, costs, color='k')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Scaled Cost (USD)')\n",
    "plt.title(f'Cost Comparison of Models for {language} Language')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwCo9941QRo2"
   },
   "source": [
    "#### üßµ Points cl√©s\n",
    "\n",
    "* **Soyez conscient de la mani√®re dont les LLM repr√©sentent votre langue**, surtout si vous utilisez des API commerciales. La fa√ßon dont votre texte est tokenis√© impacte directement le co√ªt.\n",
    "* Si vous entra√Ænez votre propre LLM, **portez une attention particuli√®re √† la tokenisation**. Vous pourriez vouloir **adapter le tokenizer √† votre langue** pour r√©duire le nombre de tokens et rendre la repr√©sentation plus compacte et efficace.\n",
    "* Des travaux r√©cents de **Cohere** explorent la cr√©ation d‚Äôun [**tokenizer universel**](https://arxiv.org/pdf/2506.10766) qui fonctionne bien pour plusieurs langues. Ce type de recherche cherche √† niveler les in√©galit√©s.\n",
    "* √Ä consulter √©galement :\n",
    "\n",
    "  * üìÑ *[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://aclanthology.org/2023.emnlp-main.614.pdf)*\n",
    "  * üìÑ *[Language Model Tokenizers Introduce Unfairness Between Languages](https://arxiv.org/pdf/2305.154255)*\n",
    "\n",
    "Ces √©tudes ont montr√© d√®s le d√©part que les tokenizers introduisent une **injustice structurelle**, en particulier pour les langues peu dot√©es en ressources. Pour cette raison, plusieurs fournisseurs commerciaux de LLM ont depuis commenc√© √† entra√Æner des **tokenizers plus repr√©sentatifs** afin de r√©duire les disparit√©s de co√ªt entre langues.\n",
    "\n",
    "En r√©sum√© : **La tokenisation n‚Äôest pas qu‚Äôun d√©tail technique, c‚Äôest une question d‚Äôacc√®s linguistique.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV3YG7QOZD-B"
   },
   "source": [
    "Voici la traduction en fran√ßais de ta conclusion :\n",
    "\n",
    "---\n",
    "\n",
    "# **Conclusion**\n",
    "\n",
    "**R√©sum√© :**\n",
    "\n",
    "Vous avez maintenant acquis les bases essentielles du fonctionnement d‚Äôun Large Language Model (LLM), depuis les m√©canismes d‚Äôattention jusqu‚Äô√† l‚Äôentra√Ænement de votre propre mod√®le ! Ces outils puissants ont le potentiel de transformer de nombreuses t√¢ches. Cependant, comme pour tout mod√®le de deep learning, leur magie r√©side dans la capacit√© √† les appliquer aux bons probl√®mes avec les bonnes donn√©es.\n",
    "\n",
    "Pr√™t¬∑e √† passer au niveau sup√©rieur ? Plongez-vous dans le fine-tuning de vos propres LLMs pour lib√©rer encore plus de potentiel ! Je recommande vivement d‚Äôexplorer le practical de l‚Äôann√©e derni√®re sur les m√©thodes de fine-tuning efficaces en param√®tres pour une vue compl√®te des techniques avanc√©es. Le voyage ne s‚Äôarr√™te pas ici ‚Äî il y a tellement plus √† d√©couvrir !\n",
    "\n",
    "Le monde des LLMs est √† votre port√©e ‚Äî lancez-vous et cr√©ez quelque chose d‚Äôincroyable ! üåüüöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Prochaines √©tapes :**\n",
    "[**Fine-tuning efficace en param√®tres des grands mod√®les de langage**](https://colab.research.google.com/drive/1_QGpdDOlKSiEyV2E1NsMQBhSqspFOs64?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "**R√©f√©rences :** Pour plus de ressources, consultez les liens r√©f√©renc√©s dans les diff√©rentes sections de ce colab.\n",
    "\n",
    "* [Article \"Attention is all you need\"](https://arxiv.org/abs/1706.03762)\n",
    "* [Qu‚Äôest-ce que les mod√®les Transformer et comment fonctionnent-ils ?](https://www.youtube.com/watch?v=qaWMOYf4ri8)\n",
    "* [Cl√©s, requ√™tes, et valeurs : la m√©canique c√©leste de l‚Äôattention](https://www.youtube.com/watch?v=RFdb2rKAqFw)\n",
    "* [Vid√©os suppl√©mentaires sur les Transformers](https://www.youtube.com/playlist?list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s)\n",
    "* [LLMs pour tous DLI2023](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
    "* [Fondations des LLM DLI2024](https://github.com/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Foundations_of_LLMs/foundations_of_llms_practical.ipynb)\n",
    "\n",
    "Pour d√©couvrir d‚Äôautres practicals du Deep Learning Indaba, rendez-vous [ici](https://github.com/deep-learning-indaba/indaba-pracs-2025).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1ndpYE50BpG"
   },
   "source": [
    "## Avis\n",
    "\n",
    "Merci de remplir ce formulaire, c‚Äôest une partie tr√®s importante des travaux pratiques. Vos retours nous aideront √† **am√©liorer les sessions et compteront √©galement pour le prix √† la fin des sessions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "OIZvkhfRz9Jz"
   },
   "outputs": [],
   "source": [
    "# @title G√©n√©rer un formulaire de commentaires. (Ex√©cuter la cellule)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/AJr8t3mzXV2WRgHy6\",\n",
    "  width=\"80%\"\n",
    "\theight=\"1200px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oglV4kHMWnIN"
   },
   "source": [
    "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7e0353a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Zgn0dW3IH9NQ",
    "ElXJ1BGzH5BJ",
    "svfeQO7VIOIu",
    "AtWMaTddww65",
    "9YPYutB1TAes",
    "WNO703V9SBcI",
    "Qkh0KgRPdDf8",
    "vY02IFQouwjN",
    "5X4tRtSZxGHg",
    "tixtBEtRPZ5n",
    "QVTduxk4PdYC",
    "WwCo9941QRo2",
    "o1ndpYE50BpG"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
